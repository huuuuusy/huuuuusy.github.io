# üîçÔ∏è Research Interests

## Research Foundation

<img align="left" src="../../images/goal.png" width="50%" height="auto" hspace="20" vspace="10">

My research has long focused on **evaluating and modeling machine vision intelligence**, covering task modeling, environment construction, evaluation techniques, and human‚Äìmachine comparisons. I firmly believe that ***<font color=DarkRed>the development of artificial intelligence is inherently intertwined with human factors</font>***. Inspired by the classical ***<font color=DarkRed>Turing Test</font>***, I have extended this concept to visual understanding, proposing the ***<font color=DarkRed>Visual Turing Test</font>*** as a human-centered framework for evaluating dynamic vision tasks. The overarching goal is to benchmark machine visual intelligence against human abilities, building **trustworthy and explainable evaluation systems** that advance us toward secure and reliable Artificial General Intelligence (AGI).

---

### 1Ô∏è‚É£ What abilities define human perception? Designing more human-like visual tasks

I take **Visual Object Tracking (VOT)** as a representative task to explore the boundaries of machine dynamic visual ability. Traditional VOT is limited by its assumption of continuous motion, which fails to align with human cognitive tracking capabilities. To overcome this, I proposed ***<font color=DarkRed>Global Instance Tracking (GIT)</font>*** ‚Äî a humanoid-inspired reformulation that shifts tracking from a short-term perceptual level to a long-term **cognitive level**. Building on this, I introduced ***<font color=DarkRed>Multi-modal GIT (MGIT)</font>*** by incorporating hierarchical semantic structures, enabling machines to perform **visual reasoning** over complex spatio-temporal causal relationships. Together, these extensions mark a transition from perceptual recognition to cognitive understanding.

---

### 2Ô∏è‚É£ What environments do humans perceive? Constructing more open and realistic visual spaces

Human environments are dynamic, continuous, and semantically rich, yet most datasets remain static and task-limited. To address this, I developed the ***[VideoCube](http://videocube.aitestunion.com/)*** benchmark by integrating **narrative theory** to decompose video content into interpretable units. Building further, I proposed ***[SOTVerse](https://huuuuusy.github.io/#SOTVerse)***, a large-scale and open **task space** (12.56M frames) that enables flexible subspace generation for evaluating **visual generalization** across diverse conditions. To tackle visual robustness in real-world dynamics, I further developed ***[BioDrone](https://huuuuusy.github.io/#BioDrone)*** ‚Äî the first bio-inspired flapping-wing drone benchmark ‚Äî providing a novel testing ground for **robust visual intelligence** under motion perturbations and environmental challenges.

---

### 3Ô∏è‚É£ How large is the human‚Äìmachine gap? Benchmarking machine vision against human ability

While computer scientists evaluate models on large datasets and neuroscientists assess humans in controlled experiments, this disciplinary gap prevents unified human‚Äìmachine evaluation. To bridge this, I constructed a unified evaluation environment based on SOTVerse, enabling **direct human‚Äìmachine comparisons** in perception, cognition, and robustness. Results reveal that recent algorithms are closing the gap with human subjects, with humans excelling in semantic understanding and machines in precision and persistence. This complementary behavior suggests the emerging potential for ***<font color=DarkRed>human‚Äìmachine collaborative intelligence</font>*** in dynamic vision.

---

<img align="left" src="../../images/3E.png" width="100%" height="auto">

This human-centered evaluation paradigm is formalized as the ***<font color=DarkRed>3E Framework</font>*** ‚Äî **Environment**, **Evaluation**, and **Executors** ‚Äî forming a closed loop that defines, measures, and evolves intelligent behavior. Machines acquire human-like abilities by iteratively performing humanoid proxy tasks within evolving environments and evaluation criteria. Through this iterative mechanism, their cognitive upper bounds are continuously improved, laying the foundation for building **evaluative, explainable, and human-aligned intelligence**.

---

## Current Research Interests

**Visual Intelligence**  
- Focuses on visual intelligence as the core channel to study how AI systems perceive, reason, and interpret in complex environments.  
- Builds interpretable and generalizable cognitive evaluation frameworks under the ‚ÄúEnvironment‚ÄìTask‚ÄìExecutor‚Äù paradigm.  
- Explores unified quantitative models for robustness, generalization, and safety, promoting a paradigm shift from performance-driven to cognition-driven evaluation.  
- Investigates human-referenced measurement principles of intelligence to support the development of human‚ÄìAI integrated cognitive systems.  

**Multimodal Cognition**  
- Investigates the structural role of vision within multimodal cognition, exploring unified mechanisms for cross-modal fusion and spatiotemporal reasoning.  
- Develops multiscale models from perception to semantics to reveal intrinsic connections among vision, language, and knowledge.  
- Studies semantic diversity, causal associations, and narrative generation to build explainable and generalizable multimodal understanding frameworks.  
- Advances visual understanding from static perception toward dynamic cognition, providing a structural foundation for next-generation multimodal intelligence.  

**AI4Edu**  
- Positions educational environments as ideal domains for studying human‚ÄìAI co-evolution and cognitive learning mechanisms.  
- Focuses on intelligent agents with personality, cognition, and social adaptability, emphasizing cognitive tracking, personalized feedback, and adaptive learning.  
- Explores multi-agent collaboration and reflective learning mechanisms, enabling human‚ÄìAI interaction with understanding, empathy, and shared growth.  
- Promotes the transformation of educational AI from an assistive tool to a cognitive partner, fostering educational equity, innovation, and sustainable learning.  

**AI4Science**  
- Explores the cognitive modeling pathways of AI in scientific discovery, experimental design, and knowledge reasoning.  
- Studies AI‚Äôs cognitive role in scientific understanding, data modeling, and hypothesis generation, abstracting cognitive principles from human reasoning.  
- Constructs integrated vision‚Äìlanguage‚Äìsymbol frameworks for scientific intelligence, bridging computational learning and human scientific cognition.  
- Advances interdisciplinary applications of AI in education, medicine, psychology, and cognitive science toward the co-evolution of artificial and human intelligence.  
