# ğŸ“ Publications

## Acceptance

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2023</div><img src='../../images/GIT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Global Instance Tracking: Locating Target More Like Humans**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [L. Huang](https://github.com/huanglianghua), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[IEEE Transactions on Pattern Analysis and Machine Intelligence](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34) (CCF-A Journal, IF=23.6)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Large-scale Benchmark Construction ğŸ“Œ Intelligent Evaluation Technology <br>
[ğŸ“ƒ Paper](https://ieeexplore.ieee.org/document/9720246/)
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/GIT.bib)
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/GIT.pdf)
[ğŸŒ Platform](http://videocube.aitestunion.com/)
[ğŸ”§ Toolkit](https://github.com/huuuuusy/videocube-toolkit) 
[ğŸ’¾ Dataset](http://videocube.aitestunion.com/downloads)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV 2023</div><img src='../../images/SOTVerse.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**SOTVerse: A User-defined Task Space of Single Object Tracking**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[International Journal of Computer Vision](https://www.springer.com/journal/11263) (CCF-A Journal, IF=19.5)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Dynamic Open Environment Construction ğŸ“Œ Visual Evaluation Technique<br>
[ğŸ“ƒ Paper](https://link.springer.com/article/10.1007/s11263-023-01908-5)
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/SOTVerse.bib)
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/SOTVerse.pdf)
[ğŸŒ Platform](http://metaverse.aitestunion.com/) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV 2023</div><img src='../../images/BioDrone.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision**<br>
[X. Zhao](https://www.xinzhaoai.com/), ***<font color=DarkRed>Shiyu Huâœ‰ï¸</font>***, [Y. Wang](https://github.com/updateforever), J. Zhang, Y. Hu, R. Liu, [H. Lin](https://www3.cs.stonybrook.edu/~hling/), [Y. Li](https://www.biostat.wisc.edu/~yli/), R. Li, K. Liu, [J. Li](http://yjsb.sinano.ac.cn/Doctor/info.aspx?itemid=920) <br>
*[International Journal of Computer Vision](https://www.springer.com/journal/11263) (CCF-A Journal, IF=19.5)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Drone-based Tracking ğŸ“Œ Visual Evaluation Technique<br>
[ğŸ“ƒ Paper](https://link.springer.com/article/10.1007/s11263-023-01937-0)
[ğŸŒ Platform](http://biodrone.aitestunion.com/) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/BioDrone.bib) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/BioDrone.pdf)
[ğŸ”§ Toolkit](https://github.com/huuuuusy/biodrone-toolkit-official) 
[ğŸ’¾ Dataset](http://biodrone.aitestunion.com/downloads) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='../../images/MGIT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and causal Relationship**<br>
***<font color=DarkRed>Shiyu Hu</font>***, D. Zhang, [M. Wu](https://github.com/wmeiqi), [X. Feng](https://github.com/XiaokunFeng), [X. Li](https://xuchen-li.github.io/), [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[the 37th Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2023) (CCF-A Conference, Poster)*<br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Long Video Understanding and Reasoning ğŸ“Œ Hierarchical Semantic Information Annotation<br>
[ğŸ“ƒ PDF](https://huuuuusy.github.io/files/MGIT.pdf)
[ğŸª§ Poster](https://huuuuusy.github.io/files/MGIT-poster.pdf)
[ğŸ“¹ Slides](https://huuuuusy.github.io/files/MGIT-Slides.pdf)
[ğŸŒ Platform](http://videocube.aitestunion.com/)
[ğŸ’¾ Dataset]([[Toolkit](https://github.com/huuuuusy/videocube-toolkit)]) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ä¸­å›½å›¾è±¡å›¾å½¢å­¦æŠ¥ 2023</div><img src='../../images/Survey23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Visual Intelligence Evaluation Techniques for Single Object Tracking: A Survey (å•ç›®æ ‡è·Ÿè¸ªä¸­çš„è§†è§‰æ™ºèƒ½è¯„ä¼°æŠ€æœ¯ç»¼è¿°)**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[Journal of Images and Graphics](http://www.cjig.cn/jig/ch/index.aspx) (ã€Šä¸­å›½å›¾è±¡å›¾å½¢å­¦æŠ¥ã€‹, CCF-B Chinese Journal)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Intelligent Evaluation Technique ğŸ“Œ AI4Science<br>
[ğŸ“ƒ Paper](http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?flag=2&file_no=202307100000002&journal_id=jig) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/JIG-survey.pdf) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2024</div><img src='../../images/ICASSP24.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Robust Single-particle Cryo-EM Image Denoising and Restoration**<br>
J. Zhang, T. Zhao, ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
*[the 49th IEEE International Conference on Acoustics, Speech, and Signal Processing](https://2024.ieeeicassp.org/) (CCF-B Conference, Poster)*<br>
ğŸ“Œ Medical Image Processing ğŸ“Œ AI4Science ğŸ“Œ Diffusion Model<br>
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/ICASSP24.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PRCV 2023</div><img src='../../images/PRCV23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**A Hierarchical Theme Recognition Model for Sandplay Therapy**<br>
[X. Feng](https://github.com/XiaokunFeng), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Chen](http://www.crise.ia.ac.cn/teachers_view.aspx?TypeId=141&Id=467&Fid=t26:141:26), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[the 6th Chinese Conference on Pattern Recognition and Computer Vision](https://www.prcv2023.cn/2023prcv) (CCF-C Conference, Poster)*<br>
ğŸ“Œ Psychological Assessment System ğŸ“Œ Gamified Assessment ğŸ“Œ AI4Science<br>
[ğŸ“ƒ Paper](https://link.springer.com/chapter/10.1007/978-981-99-8462-6_20) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/PRCV23.bib) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/PRCV23.pdf)
[ğŸ”– Supplementary](https://huuuuusy.github.io/files/PRCV23-Supp.pdf)
[ğŸª§ Poster](https://huuuuusy.github.io/files/PRCV23-poster.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CSAI 2023</div><img src='../../images/CSAI23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Rethinking Similar Object Interference in Single Object Tracking**<br>
[Y. Wang](https://github.com/updateforever), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
*[the 7th International Conference on Computer Science and Artificial Intelligence](http://www.csai.org/) (EI Conference, **Oral**)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Similar Object Interference ğŸ“Œ Data Mining<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Neurocomputing 2022</div><img src='../../images/Neu22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Revisiting Instance Search: A New Benchmark Using Cycle Self-training**<br>
[Y. Zhang](https://wesleyzhang1991.github.io/), [C. Liu](https://scholar.google.com/citations?user=atOfOgMAAAAJ&hl=zh-CN&oi=sra), [W. Chen](https://scholar.google.com/citations?user=KWVlYaMAAAAJ&hl=zh-CN&oi=sra), [X. Xu](https://scholar.google.com/citations?user=nJc6BvgAAAAJ&hl=zh-CN&oi=sra), [F. Wang](https://scholar.google.com/citations?user=WCRGTHsAAAAJ), [H. Li](https://scholar.google.com/citations?user=pHN-QIwAAAAJ&hl=zh-CN&oi=sra), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
*[Neurocomputing](https://www.sciencedirect.com/journal/neurocomputing)  (CCF-C Journal, IF=6)*<br>
ğŸ“Œ Video Instance Search ğŸ“Œ Benchmark Construction  ğŸ“Œ Data Mining<br>
[ğŸ“ƒ Paper](https://www.sciencedirect.com/science/article/abs/pii/S0925231222007445) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/Neu22.bib) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/Neu22.pdf) 
[ğŸŒ Project](https://github.com/Instance-Search/) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">å›¾å­¦å­¦æŠ¥ 2021</div><img src='../../images/VTT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Visual Turing: The Next Development of Computer Vision in The View of Human-computer Gaming (è§†è§‰å›¾çµï¼šä»äººæœºå¯¹æŠ—çœ‹è®¡ç®—æœºè§†è§‰ä¸‹ä¸€æ­¥å‘å±•)**<br>
[K. Huang](https://people.ucas.ac.cn/~huangkaiqi), [X. Zhao](https://www.xinzhaoai.com/), [Q. Li](https://scholar.google.com/citations?user=7xmxBagAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***<br>
*[Journal of Graphics](http://www.txxb.com.cn/CN/2095-302X/home.shtml) (ã€Šå›¾å­¦å­¦æŠ¥ã€‹, CCF-C Chinese Journal)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Intelligent Evaluation Technique  ğŸ“Œ AI4Science<br>
[ğŸ“ƒ Paper](http://www.txxb.com.cn/CN/10.11996/JG.j.2095-302X.2021030339) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/VTT.bib) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/VTT.pdf)

</div>
</div>

## Under Review

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">TCSVT 2024</div><img src='../../images/SOI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Target or Distractor? Rethinking Similar Object Interference in Single Object Tracking**<br>
[Y. Wang](https://github.com/updateforever), ***<font color=DarkRed>Shiyu Hu</font>***, D. Zhang, [M. Wu](https://github.com/wmeiqi), [T. Yao](http://tingyao.deepfun.club/), [Y. Wang](https://scholar.google.com/citations?user=3nMDEBYAAAAJ), [L. Chen](https://sie.bit.edu.cn/szdw/jsml/ldjsyjsj/zgzcl/06c26b3ebaae4db981aaa388c660c8b5.htm), [X. Zhao](https://www.xinzhaoai.com/) <br>
*[IEEE Transactions on Circuits and Systems for Video Technology](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76) (CCF-B Journal, IF=8.4, Under Review)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Similar Object Interference ğŸ“Œ Data Mining<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">TCSVT 2024</div><img src='../../images/AWCV.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Finger in Camera Speaks Everything: Unconstrained Air-Writing for Real-World**<br>
[M. Wu](https://github.com/wmeiqi), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi), [Y. Cai](https://teacher.bupt.edu.cn/caiyuanqiang/zh_CN/index.htm), ***<font color=DarkRed>Shiyu Hu</font>***, [Y. Zhao](https://callsys.github.io/zhaoyuzhong.github.io-main/), [W. Wang](https://people.ucas.ac.cn/~wqwang?language=en) <br>
*[IEEE Transactions on Circuits and Systems for Video Technology](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76) (CCF-B Journal, IF=8.4, Under Review)*<br>
ğŸ“Œ Air-writing Technique ğŸ“Œ Benchmark Construction ğŸ“Œ Human-machine Interaction<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">IJCAI 2024</div><img src='../../images/MemTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Remembering Target More Like Humans: A Robust Visual-Language Tracker with Adaptive Prompts**<br>
[X. Feng](https://github.com/XiaokunFeng), [X. Li](https://xuchen-li.github.io/), ***<font color=DarkRed>Shiyu Hu</font>***, D. Zhang, [M. Wu](https://github.com/wmeiqi), [X. Chen](http://www.crise.ia.ac.cn/teachers_view.aspx?TypeId=141&Id=467&Fid=t26:141:26), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
*[the 33rd International Joint Conference on Artificial Intelligence](https://www.ijcai.org/) (CCF-A Conference, Under Review)*<br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Human-like Memory Modeling ğŸ“Œ Adaptive Prompts<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">å¿ƒç†å­¦æŠ¥ 2024</div><img src='../../images/Sandplay.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Intelligent Psychological Assessment with Sandplay based on Evidence-Centered Design Theory (åŸºäºè¯æ®ä¸­å¿ƒè®¾è®¡ç†è®ºçš„æ™ºèƒ½å¿ƒç†æ²™ç›˜æµ‹è¯„ç³»ç»Ÿ)**<br>
Y. Ren, [X. Feng](https://github.com/XiaokunFeng), ***<font color=DarkRed>Shiyu Hu</font>***, Y. Kang, C. Yan, Y. Zeng, [L. Wang](https://people.ucas.ac.cn/~wanglg), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
*[Acta Psychologica Sinica](https://journal.psych.ac.cn/xlxb/CN/0439-755X/home.shtml) (ã€Šå¿ƒç†å­¦æŠ¥ã€‹, CSSCI Journal, Top-1 Psychological Journal in China, Under Review)*<br>
ğŸ“Œ Psychological Assessment System ğŸ“Œ Gamified Assessment ğŸ“Œ AI4Science<br>

</div>
</div>


# âš™ï¸ Projects

> The list here mainly includes engineering projects and ongoing academic projects, while more academic projects have already been published in the form of research papers. Please refer to the [ğŸ“ Publications](https://huuuuusy.github.io/#-publications) for more information.

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">2018.03-2018.11</div><img src='../../images/DarknetCross.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Darknet-Cross: Light-weight Deep Learning Framework for Heterogeneous Computing**<br>
ğŸ“Œ High-performance Computing ğŸ“Œ Heterogeneous Computing ğŸ“Œ Deep learning Framework<br>
- [Darknet-Cross](https://github.com/huuuuusy/Darknet-Cross) is a lightweight deep learning framework, mainly based on the open-source deep learning algorithm library Darknet and yolov2_light, and it has been successfully ported to mobile devices through cross-compilation. This framework enables efficient algorithm inference using mobile GPUs.
- Darknet-Cross supports algorithm acceleration processing on various platforms (e.g., Android and Ubuntu) and various GPUs (e.g., Nvidia GTX1070 and Adreno 630).
- The work is a part of my master's thesis at the University of Hong Kong (thesis defense grade: A+).

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">2019.05 - 2019.10</div><img src='../../images/SkinColor.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**A Skin Color Detection System without Color Atla**<br>
ğŸ“Œ Color Constancy ğŸ“Œ Skin Color Detection ğŸ“Œ Illumination Estimation<br>
- Under 18 different environmental lighting conditions and with 4 combinations of smartphone parameters, skin color data was collected from 110 participants. The skin color dataset consists of 7,920 images, with the testing results from CK Company's MPA9 skin color detector serving as the ground truth for user skin colors.
- Using an elliptical skin model, the essential skin regions are extracted from the images. The open-source color constancy model, FC4, is employed to recover the environmental lighting conditions. Subsequently, the skin color detection results for users are calculated using SVR regression.
- The related work has been successfully deployed in Huawei's official mobile application 'Mirror' for its AI skin testing function.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">2020.11 - 2021.03</div><img src='../../images/tracking-result.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**A Project for Cell Tracking Based on Deep Learning Method**<br>
ğŸ“Œ Medical Image Processing ğŸ“Œ AI4Science ğŸ“Œ Cell Segmentation and Tracking<br>
- This method follows the tracking by detection paradigm and combines per-frame CNN prediction for cell segmentation with a Siamese network for cell tracking. 
- This project was submitted to the cell tracking challenge in Mar. 2021, and maintains the second place in the Fluo-C2FL-MSC+ dataset and the third place in the Fluo-C2FL-Huh7 dataset (statistics by Oct. 2023).

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">2022.04 - Now</div><img src='../../images/VTT-ICLR.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Intelligent Evaluation Techniques for Visual Object Tracking Based on Visual Turing Test**<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Intelligent Evaluation Technique ğŸ“Œ AI4Science<br>
- This work proposes the visual Turing test evaluation paradigm by incorporating the concept of the Turing test, enabling a comprehensive assessment of the visual intelligence of algorithms in comparison to human visual capabilities.
- A controlled experimental environment has been developed to facilitate a fair comparison of dynamic visual abilities between humans and machines. This environment incorporates the perceptual and cognitive capabilities that task objects necessitate during the execution of dynamic visual tasks.
- A suitable task object is selected to conduct tests on human-machine dynamic visual abilities, involving 20 representative algorithms and 15 human subjects.
- A universally applicable multi-scale dynamic visual task evaluation framework has been designed. This framework employs center point distance to assess and analyze tasks at three distinct scales, namely frame-level, sequence-level, and group-level.
- Two Chinese review papers were published in 2021 and 2023. The experimental content and main conclusions are being finalized, preparing for submission to the Cell Patterns journal.

</div>
</div>