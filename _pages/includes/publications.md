# 📝 Publications

## Acceptance

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2023</div><img src='../../images/GIT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Global Instance Tracking: Locating Target More Like Humans**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [L. Huang](https://github.com/huanglianghua), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[IEEE Transactions on Pattern Analysis and Machine Intelligence](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34) (CCF-A Journal, IF=23.6)*<br>
📌 Visual Object Tracking 📌 Large-scale Benchmark Construction 📌 Intelligent Evaluation Technology <br>
[📃 Paper](https://ieeexplore.ieee.org/document/9720246/)
[🗒 bibTex](https://huuuuusy.github.io/files/GIT.bib)
[📑 PDF](https://huuuuusy.github.io/files/GIT.pdf)
[🌐 Platform](http://videocube.aitestunion.com/)
[🔧 Toolkit](https://github.com/huuuuusy/videocube-toolkit) 
[💾 Dataset](http://videocube.aitestunion.com/downloads)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV 2023</div><img src='../../images/SOTVerse.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**SOTVerse: A User-defined Task Space of Single Object Tracking**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[International Journal of Computer Vision](https://www.springer.com/journal/11263) (CCF-A Journal, IF=19.5)*<br>
📌 Visual Object Tracking 📌 Dynamic Open Environment Construction 📌 Visual Evaluation Technique<br>
[📃 Paper](https://link.springer.com/article/10.1007/s11263-023-01908-5)
[🗒 bibTex](https://huuuuusy.github.io/files/SOTVerse.bib)
[📑 PDF](https://huuuuusy.github.io/files/SOTVerse.pdf)
[🌐 Platform](http://metaverse.aitestunion.com/) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV 2023</div><img src='../../images/BioDrone.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision**<br>
[X. Zhao](https://www.xinzhaoai.com/), ***<font color=DarkRed>Shiyu Hu✉️</font>***, [Y. Wang](https://github.com/updateforever), J. Zhang, Y. Hu, R. Liu, [H. Lin](https://www3.cs.stonybrook.edu/~hling/), [Y. Li](https://www.biostat.wisc.edu/~yli/), R. Li, K. Liu, [J. Li](http://yjsb.sinano.ac.cn/Doctor/info.aspx?itemid=920) <br>
*[International Journal of Computer Vision](https://www.springer.com/journal/11263) (CCF-A Journal, IF=19.5)*<br>
📌 Visual Object Tracking 📌 Drone-based Tracking 📌 Visual Evaluation Technique<br>
[📃 Paper](https://link.springer.com/article/10.1007/s11263-023-01937-0)
[🌐 Platform](http://biodrone.aitestunion.com/) 
[🗒 bibTex](https://huuuuusy.github.io/files/BioDrone.bib) 
[📑 PDF](https://huuuuusy.github.io/files/BioDrone.pdf)
[🔧 Toolkit](https://github.com/huuuuusy/biodrone-toolkit-official) 
[💾 Dataset](http://biodrone.aitestunion.com/downloads) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='../../images/MGIT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and causal Relationship**<br>
***<font color=DarkRed>Shiyu Hu</font>***, D. Zhang, [M. Wu](https://github.com/wmeiqi), [X. Feng](https://github.com/XiaokunFeng), [X. Li](https://xuchen-li.github.io/), [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[the 37th Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2023) (CCF-A Conference, Poster)*<br>
📌 Visual Language Tracking 📌 Long Video Understanding and Reasoning 📌 Hierarchical Semantic Information Annotation<br>
[📃 PDF](https://huuuuusy.github.io/files/MGIT.pdf)
[🪧 Poster](https://huuuuusy.github.io/files/MGIT-poster.pdf)
[📹 Slides](https://huuuuusy.github.io/files/MGIT-Slides.pdf)
[🌐 Platform](http://videocube.aitestunion.com/)
[💾 Dataset]([[Toolkit](https://github.com/huuuuusy/videocube-toolkit)]) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">中国图象图形学报 2023</div><img src='../../images/Survey23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Visual Intelligence Evaluation Techniques for Single Object Tracking: A Survey (单目标跟踪中的视觉智能评估技术综述)**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[Journal of Images and Graphics](http://www.cjig.cn/jig/ch/index.aspx) (《中国图象图形学报》, CCF-B Chinese Journal)*<br>
📌 Visual Object Tracking 📌 Intelligent Evaluation Technique 📌 AI4Science<br>
[📃 Paper](http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?flag=2&file_no=202307100000002&journal_id=jig) 
[📑 PDF](https://huuuuusy.github.io/files/JIG-survey.pdf) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2024</div><img src='../../images/ICASSP24.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Robust Single-particle Cryo-EM Image Denoising and Restoration**<br>
J. Zhang, T. Zhao, ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
*[the 49th IEEE International Conference on Acoustics, Speech, and Signal Processing](https://2024.ieeeicassp.org/) (CCF-B Conference, Poster)*<br>
📌 Medical Image Processing 📌 AI4Science 📌 Diffusion Model<br>
[📑 PDF](https://huuuuusy.github.io/files/ICASSP24.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PRCV 2023</div><img src='../../images/PRCV23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**A Hierarchical Theme Recognition Model for Sandplay Therapy**<br>
[X. Feng](https://github.com/XiaokunFeng), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Chen](http://www.crise.ia.ac.cn/teachers_view.aspx?TypeId=141&Id=467&Fid=t26:141:26), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[the 6th Chinese Conference on Pattern Recognition and Computer Vision](https://www.prcv2023.cn/2023prcv) (CCF-C Conference, Poster)*<br>
📌 Psychological Assessment System 📌 Gamified Assessment 📌 AI4Science<br>
[📃 Paper](https://link.springer.com/chapter/10.1007/978-981-99-8462-6_20) 
[🗒 bibTex](https://huuuuusy.github.io/files/PRCV23.bib) 
[📑 PDF](https://huuuuusy.github.io/files/PRCV23.pdf)
[🔖 Supplementary](https://huuuuusy.github.io/files/PRCV23-Supp.pdf)
[🪧 Poster](https://huuuuusy.github.io/files/PRCV23-poster.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CSAI 2023</div><img src='../../images/CSAI23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Rethinking Similar Object Interference in Single Object Tracking**<br>
[Y. Wang](https://github.com/updateforever), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
*[the 7th International Conference on Computer Science and Artificial Intelligence](http://www.csai.org/) (EI Conference, **Oral**)*<br>
📌 Visual Object Tracking 📌 Similar Object Interference 📌 Data Mining<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Neurocomputing 2022</div><img src='../../images/Neu22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Revisiting Instance Search: A New Benchmark Using Cycle Self-training**<br>
[Y. Zhang](https://wesleyzhang1991.github.io/), [C. Liu](https://scholar.google.com/citations?user=atOfOgMAAAAJ&hl=zh-CN&oi=sra), [W. Chen](https://scholar.google.com/citations?user=KWVlYaMAAAAJ&hl=zh-CN&oi=sra), [X. Xu](https://scholar.google.com/citations?user=nJc6BvgAAAAJ&hl=zh-CN&oi=sra), [F. Wang](https://scholar.google.com/citations?user=WCRGTHsAAAAJ), [H. Li](https://scholar.google.com/citations?user=pHN-QIwAAAAJ&hl=zh-CN&oi=sra), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
*[Neurocomputing](https://www.sciencedirect.com/journal/neurocomputing)  (CCF-C Journal, IF=6)*<br>
📌 Video Instance Search 📌 Benchmark Construction  📌 Data Mining<br>
[📃 Paper](https://www.sciencedirect.com/science/article/abs/pii/S0925231222007445) 
[🗒 bibTex](https://huuuuusy.github.io/files/Neu22.bib) 
[📑 PDF](https://huuuuusy.github.io/files/Neu22.pdf) 
[🌐 Project](https://github.com/Instance-Search/) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">图学学报 2021</div><img src='../../images/VTT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Visual Turing: The Next Development of Computer Vision in The View of Human-computer Gaming (视觉图灵：从人机对抗看计算机视觉下一步发展)**<br>
[K. Huang](https://people.ucas.ac.cn/~huangkaiqi), [X. Zhao](https://www.xinzhaoai.com/), [Q. Li](https://scholar.google.com/citations?user=7xmxBagAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***<br>
*[Journal of Graphics](http://www.txxb.com.cn/CN/2095-302X/home.shtml) (《图学学报》, CCF-C Chinese Journal)*<br>
📌 Visual Object Tracking 📌 Intelligent Evaluation Technique  📌 AI4Science<br>
[📃 Paper](http://www.txxb.com.cn/CN/10.11996/JG.j.2095-302X.2021030339) 
[🗒 bibTex](https://huuuuusy.github.io/files/VTT.bib) 
[📑 PDF](https://huuuuusy.github.io/files/VTT.pdf)

</div>
</div>

## Under Review

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">TCSVT 2024</div><img src='../../images/SOI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Target or Distractor? Rethinking Similar Object Interference in Single Object Tracking**<br>
[Y. Wang](https://github.com/updateforever), ***<font color=DarkRed>Shiyu Hu</font>***, D. Zhang, [M. Wu](https://github.com/wmeiqi), [T. Yao](http://tingyao.deepfun.club/), [Y. Wang](https://scholar.google.com/citations?user=3nMDEBYAAAAJ), [L. Chen](https://sie.bit.edu.cn/szdw/jsml/ldjsyjsj/zgzcl/06c26b3ebaae4db981aaa388c660c8b5.htm), [X. Zhao](https://www.xinzhaoai.com/) <br>
*[IEEE Transactions on Circuits and Systems for Video Technology](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76) (CCF-B Journal, IF=8.4, Under Review)*<br>
📌 Visual Object Tracking 📌 Similar Object Interference 📌 Data Mining<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">TCSVT 2024</div><img src='../../images/AWCV.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Finger in Camera Speaks Everything: Unconstrained Air-Writing for Real-World**<br>
[M. Wu](https://github.com/wmeiqi), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi), [Y. Cai](https://teacher.bupt.edu.cn/caiyuanqiang/zh_CN/index.htm), ***<font color=DarkRed>Shiyu Hu</font>***, [Y. Zhao](https://callsys.github.io/zhaoyuzhong.github.io-main/), [W. Wang](https://people.ucas.ac.cn/~wqwang?language=en) <br>
*[IEEE Transactions on Circuits and Systems for Video Technology](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76) (CCF-B Journal, IF=8.4, Under Review)*<br>
📌 Air-writing Technique 📌 Benchmark Construction 📌 Human-machine Interaction<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">IJCAI 2024</div><img src='../../images/MemTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Remembering Target More Like Humans: A Robust Visual-Language Tracker with Adaptive Prompts**<br>
[X. Feng](https://github.com/XiaokunFeng), [X. Li](https://xuchen-li.github.io/), ***<font color=DarkRed>Shiyu Hu</font>***, D. Zhang, [M. Wu](https://github.com/wmeiqi), [X. Chen](http://www.crise.ia.ac.cn/teachers_view.aspx?TypeId=141&Id=467&Fid=t26:141:26), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
*[the 33rd International Joint Conference on Artificial Intelligence](https://www.ijcai.org/) (CCF-A Conference, Under Review)*<br>
📌 Visual Language Tracking 📌 Human-like Memory Modeling 📌 Adaptive Prompts<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">心理学报 2024</div><img src='../../images/Sandplay.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Intelligent Psychological Assessment with Sandplay based on Evidence-Centered Design Theory (基于证据中心设计理论的智能心理沙盘测评系统)**<br>
Y. Ren, [X. Feng](https://github.com/XiaokunFeng), ***<font color=DarkRed>Shiyu Hu</font>***, Y. Kang, C. Yan, Y. Zeng, [L. Wang](https://people.ucas.ac.cn/~wanglg), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
*[Acta Psychologica Sinica](https://journal.psych.ac.cn/xlxb/CN/0439-755X/home.shtml) (《心理学报》, CSSCI Journal, Top-1 Psychological Journal in China, Under Review)*<br>
📌 Psychological Assessment System 📌 Gamified Assessment 📌 AI4Science<br>

</div>
</div>


# ⚙️ Projects

> The list here mainly includes engineering projects and ongoing academic projects, while more academic projects have already been published in the form of research papers. Please refer to the [📝 Publications](https://huuuuusy.github.io/#-publications) for more information.

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">2018.03-2018.11</div><img src='../../images/DarknetCross.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Darknet-Cross: Light-weight Deep Learning Framework for Heterogeneous Computing**<br>
📌 High-performance Computing 📌 Heterogeneous Computing 📌 Deep learning Framework<br>
- [Darknet-Cross](https://github.com/huuuuusy/Darknet-Cross) is a lightweight deep learning framework, mainly based on the open-source deep learning algorithm library Darknet and yolov2_light, and it has been successfully ported to mobile devices through cross-compilation. This framework enables efficient algorithm inference using mobile GPUs.
- Darknet-Cross supports algorithm acceleration processing on various platforms (e.g., Android and Ubuntu) and various GPUs (e.g., Nvidia GTX1070 and Adreno 630).
- The work is a part of my master's thesis at the University of Hong Kong (thesis defense grade: A+).

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">2019.05 - 2019.10</div><img src='../../images/SkinColor.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**A Skin Color Detection System without Color Atla**<br>
📌 Color Constancy 📌 Skin Color Detection 📌 Illumination Estimation<br>
- Under 18 different environmental lighting conditions and with 4 combinations of smartphone parameters, skin color data was collected from 110 participants. The skin color dataset consists of 7,920 images, with the testing results from CK Company's MPA9 skin color detector serving as the ground truth for user skin colors.
- Using an elliptical skin model, the essential skin regions are extracted from the images. The open-source color constancy model, FC4, is employed to recover the environmental lighting conditions. Subsequently, the skin color detection results for users are calculated using SVR regression.
- The related work has been successfully deployed in Huawei's official mobile application 'Mirror' for its AI skin testing function.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">2020.11 - 2021.03</div><img src='../../images/tracking-result.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**A Project for Cell Tracking Based on Deep Learning Method**<br>
📌 Medical Image Processing 📌 AI4Science 📌 Cell Segmentation and Tracking<br>
- This method follows the tracking by detection paradigm and combines per-frame CNN prediction for cell segmentation with a Siamese network for cell tracking. 
- This project was submitted to the cell tracking challenge in Mar. 2021, and maintains the second place in the Fluo-C2FL-MSC+ dataset and the third place in the Fluo-C2FL-Huh7 dataset (statistics by Oct. 2023).

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">2022.04 - Now</div><img src='../../images/VTT-ICLR.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Intelligent Evaluation Techniques for Visual Object Tracking Based on Visual Turing Test**<br>
📌 Visual Object Tracking 📌 Intelligent Evaluation Technique 📌 AI4Science<br>
- This work proposes the visual Turing test evaluation paradigm by incorporating the concept of the Turing test, enabling a comprehensive assessment of the visual intelligence of algorithms in comparison to human visual capabilities.
- A controlled experimental environment has been developed to facilitate a fair comparison of dynamic visual abilities between humans and machines. This environment incorporates the perceptual and cognitive capabilities that task objects necessitate during the execution of dynamic visual tasks.
- A suitable task object is selected to conduct tests on human-machine dynamic visual abilities, involving 20 representative algorithms and 15 human subjects.
- A universally applicable multi-scale dynamic visual task evaluation framework has been designed. This framework employs center point distance to assess and analyze tasks at three distinct scales, namely frame-level, sequence-level, and group-level.
- Two Chinese review papers were published in 2021 and 2023. The experimental content and main conclusions are being finalized, preparing for submission to the Cell Patterns journal.

</div>
</div>