# ğŸ“ Publications

<!-- ## Paper Summary

### Journal

- **TPAMI**: IEEE Transactions on Pattern Analysis and Machine Intelligence (CCF-A Journal, Top-1 journal in computer vision, IF=20.8). ***<font color=DarkRed>AcceptanceÃ—1 (first authorÃ—1)</font>***
- **IJCV**: International Journal of Computer Vision (CCF-A Journal, Top-2 journal in computer vision, IF=11.6). ***<font color=DarkRed>AcceptanceÃ—2 (first authorÃ—1, corresponding-authorÃ—1)</font>***
- **TCSVT**: IEEE Transactions on Circuits and Systems for Video Technology (CCF-B Journal, IF=8.3). ***<font color=DarkRed>AcceptanceÃ—1, under reviewÃ—1</font>***
- **JIG**: Journal of Images and Graphics (ã€Šä¸­å›½å›¾è±¡å›¾å½¢å­¦æŠ¥ã€‹, CCF-B Chinese Journal). ***<font color=DarkRed>AcceptanceÃ—1 (first authorÃ—1)</font>***
- **JOG**: Journal of Graphics (ã€Šå›¾å­¦å­¦æŠ¥ã€‹, CCF-C Chinese Journal). ***<font color=DarkRed>AcceptanceÃ—1</font>***
- **Neu**: Neurocomputing (CCF-C Journal, IF=5.5). ***<font color=DarkRed>AcceptanceÃ—1</font>***
- **CMHJ**: Chinese Mental Health Journal (ã€Šä¸­å›½å¿ƒç†å«ç”Ÿæ‚å¿—ã€‹, CSSCI Journal, Top Psychological Journal in China) ***<font color=DarkRed>AcceptanceÃ—1</font>***
- **APS**: Acta Psychologica Sinica (ã€Šå¿ƒç†å­¦æŠ¥ã€‹, CSSCI Journal, Top-1 Psychological Journal in China). ***<font color=DarkRed>Under reviewÃ—1</font>***

### Conference

- **NeurIPS**: Conference on Neural Information Processing Systems (CCF-A Conference). ***<font color=DarkRed>AcceptanceÃ—3 (first authorÃ—1)</font>***
- **ICLR**: International Conference on Learning Representations (CAAI-A Conference). ***<font color=DarkRed>Under reviewÃ—3 (first authorÃ—2)</font>***
- **AAAI**: Annual AAAI Conference on Artificial Intelligence (CCF-A Conference). ***<font color=DarkRed>Under reviewÃ—1</font>***
- **NeurIPSW**: Workshop in Conference on Neural Information Processing Systems (CCF-A Conference workshop). ***<font color=DarkRed>Under reviewÃ—1</font>***
- **CVPRW**: Workshop in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CCF-A Conference workshop). ***<font color=DarkRed>AcceptanceÃ—1 (oral & best paper honorable mentionÃ—1)</font>***
- **ICASSP**: IEEE International Conference on Acoustics, Speech, and Signal Processing (CCF-B Conference). ***<font color=DarkRed>AcceptanceÃ—1, under reviewÃ—1</font>***
- **PRCV**: Chinese Conference on Pattern Recognition and Computer Vision (CCF-C Conference). ***<font color=DarkRed>AcceptanceÃ—2</font>***
- **CSAI**: International Conference on Computer Science and Artificial Intelligence (EI Conference). ***<font color=DarkRed>AcceptanceÃ—1 (oralÃ—1)</font>*** -->

## Book


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Springer 2025</div><img src='../../images/SpringerBook.webp' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='SpringerBook'></span>

**Visual Object Tracking: An Evaluation Perspective**<br>
[X. Zhao](https://www.xinzhaoai.com/), ***<font color=DarkRed>Shiyu Hu</font>***,  [X. Yin](https://scce.ustb.edu.cn/shiziduiwu/jiaoshixinxi/2018-04-12/62.html)<br>
*[Springer, Part of the book series: Advances in Computer Vision and Pattern Recognition (ACVPR)](https://www.springer.com/series/4205)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Intelligent Evaluation Technology <br>
[ğŸ“ƒ Book](https://link.springer.com/book/9789819645572)
<!-- [ğŸ—’ bibTex](https://huuuuusy.github.io/files/GIT.bib) -->
<!-- [ğŸ“‘ PDF](https://huuuuusy.github.io/files/GIT.pdf) -->

</div>
</div>


## Acceptance

<!-- å‰5ç¯‡ä»£è¡¨ä½œæŒ‰ç…§å›ºå®šé¡ºåºæ’åˆ— -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2023</div><img src='../../images/GIT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='GIT'></span>

**Global Instance Tracking: Locating Target More Like Humans**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [L. Huang](https://github.com/huanglianghua), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[IEEE Transactions on Pattern Analysis and Machine Intelligence](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34) (CCF-A Journal)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Large-scale Benchmark Construction ğŸ“Œ Intelligent Evaluation Technology <br>
[ğŸ“ƒ Paper](https://ieeexplore.ieee.org/document/9720246/)
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/GIT.pdf)
[ğŸª§ Poster](https://huuuuusy.github.io/files/VALSE24Poster-364.pdf)
[ğŸŒ Platform](http://videocube.aitestunion.com/)
[ğŸ”§ Toolkit](https://github.com/huuuuusy/videocube-toolkit) 
[ğŸ’¾ Dataset](http://videocube.aitestunion.com/downloads)
<!-- [ğŸ—’ bibTex](https://huuuuusy.github.io/files/GIT.bib) -->

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV 2024</div><img src='../../images/SOTVerse.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='SOTVerse'></span>

**SOTVerse: A User-defined Task Space of Single Object Tracking**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[International Journal of Computer Vision](https://www.springer.com/journal/11263) (CCF-A Journal)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Dynamic Open Environment Construction ğŸ“Œ 3E Paradigm<br>
[ğŸ“ƒ Paper](https://link.springer.com/article/10.1007/s11263-023-01908-5)
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/SOTVerse.pdf)
[ğŸŒ Platform](http://metaverse.aitestunion.com/) 
<!-- [ğŸ—’ bibTex](https://huuuuusy.github.io/files/SOTVerse.bib) -->

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV 2024</div><img src='../../images/BioDrone.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='BioDrone'></span>

**BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision**<br>
[X. Zhao](https://www.xinzhaoai.com/), ***<font color=DarkRed>Shiyu Huâœ‰ï¸</font>***, [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), J. Zhang, Y. Hu, R. Liu, [H. Lin](https://www3.cs.stonybrook.edu/~hling/), [Y. Li](https://www.biostat.wisc.edu/~yli/), R. Li, K. Liu, [J. Li](http://yjsb.sinano.ac.cn/Doctor/info.aspx?itemid=920) <br>
*[International Journal of Computer Vision](https://www.springer.com/journal/11263) (CCF-A Journal)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Drone-based Tracking ğŸ“Œ Visual Robustness<br>
[ğŸ“ƒ Paper](https://link.springer.com/article/10.1007/s11263-023-01937-0)
[ğŸŒ Platform](http://biodrone.aitestunion.com/) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/BioDrone.pdf)
[ğŸ”§ Toolkit](https://github.com/huuuuusy/biodrone-toolkit-official) 
[ğŸ’¾ Dataset](http://biodrone.aitestunion.com/downloads) 
<!-- [ğŸ—’ bibTex](https://huuuuusy.github.io/files/BioDrone.bib)  -->
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='../../images/MGIT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MGIT'></span>

**A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and causal Relationship**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [X. Li](https://github.com/Xuchen-Li), [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2023) (CCF-A Conference, Poster)*<br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Long Video Understanding and Reasoning ğŸ“Œ Hierarchical Semantic Information Annotation<br>
[ğŸ“ƒ Paper](https://proceedings.nips.cc/paper_files/paper/2023/hash/4ea14e6090343523ddcd5d3ca449695f-Abstract-Datasets_and_Benchmarks.html) 
[ğŸ“ƒ PDF](https://huuuuusy.github.io/files/MGIT.pdf)
[ğŸª§ Poster](https://huuuuusy.github.io/files/MGIT-poster.pdf)
[ğŸ“¹ Slides](https://huuuuusy.github.io/files/MGIT-Slides.pdf)
[ğŸŒ Platform](http://videocube.aitestunion.com/)
[ğŸ”§ Toolkit](https://github.com/huuuuusy/videocube-toolkit) 
[ğŸ’¾ Dataset](http://videocube.aitestunion.com/downloads)
<!-- [ğŸ—’ bibTex](https://huuuuusy.github.io/files/MGIT.bib) -->
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ä¸­å›½å›¾è±¡å›¾å½¢å­¦æŠ¥ 2023</div><img src='../../images/Survey23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='JIG-survey'></span>

**Visual Intelligence Evaluation Techniques for Single Object Tracking: A Survey (å•ç›®æ ‡è·Ÿè¸ªä¸­çš„è§†è§‰æ™ºèƒ½è¯„ä¼°æŠ€æœ¯ç»¼è¿°)**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[Journal of Images and Graphics](http://www.cjig.cn/jig/ch/index.aspx) (ã€Šä¸­å›½å›¾è±¡å›¾å½¢å­¦æŠ¥ã€‹, CCF-B Chinese Journal)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Intelligent Evaluation Technique ğŸ“Œ AI4Science<br>
[ğŸ“ƒ Paper](http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?flag=2&file_no=202307100000002&journal_id=jig) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/JIG-survey.pdf) 

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICMR 2025</div><img src='../../images/DARTer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='DARTer'></span>

**DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking**<br>
[X. Li\*](https://github.com/XuzhaoLi), [X. Li\*](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Huâœ‰ï¸</font>***, 
*[International Conference on Multimedia Retrieval](https://www.icmr-2025.org/) (CCF-B Conference)*<br>
ğŸ“Œ Nighttime UAVs Tracking ğŸ“Œ Dark Feature Blending ğŸ“Œ Dynamic Feature Activation <br>
[ğŸ“ƒ Paper](https://www.arxiv.org/abs/2505.00752)
[ğŸ“‘ PDF](https://www.arxiv.org/pdf/2505.00752)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IET-CVI 2025</div><img src='../../images/MSAD.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MSAD'></span>

**Improved SAR Aircraft Detection Algorithm Based on Visual State Space Models**<br>
Y. Wang, J. Zhang, [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), ***<font color=DarkRed>Shiyu Huâœ‰ï¸</font>***, B. Shen, Z. Hou, [W. Zhou](https://scholar.google.com/citations?user=r8x76hUAAAAJ). 
*[IET Computer Vision](https://digital-library.theiet.org/journal/iet-cvi) (CCF-C Journal)*<br>
ğŸ“Œ Synthetic Aperture Radar ğŸ“Œ State Space Models ğŸ“Œ Aircraft Object Detection <br>
<!-- [ğŸ“ƒ Paper](https://www.arxiv.org/abs/2505.00752) -->
<!-- [ğŸ“‘ PDF](https://www.arxiv.org/pdf/2505.00752) -->
</div>
</div>


<!-- åˆä½œè®ºæ–‡æŒ‰æ—¶é—´é¡ºåºæ’åˆ— -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2025</div><img src='../../images/CSTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='CSTrack'></span>

**CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Li](https://github.com/Xuchen-Li), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
*[International Conference on Machine Learning](https://icml.cc/) (CCF-A Conference, Poster)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Multi-modal Learning <br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='../../images/CPDTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='CPDTrack'></span>

**Beyond Accuracy: Tracking more like Human via Visual Search**<br>
[D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [X. Li](https://github.com/Xuchen-Li), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
*[Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2024) (CCF-A Conference, Poster)*<br>
ğŸ“Œ  Visual Object Tracking ğŸ“Œ Visual Search Mechanism ğŸ“Œ Visual Turing Test<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='../../images/MemVLT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MemVLT'></span>

**MemVLT: Vision-Language Tracking with Adaptive Memory-based Prompts**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [X. Li](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Hu</font>***, [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
*[Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2024) (CCF-A Conference, Poster)*<br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Human-like Memory Modeling ğŸ“Œ Adaptive Prompts<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRW 2024</div><img src='../../images/DTLLM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='DTLLM'></span>

**Diverse Text Generation for Visual Language Tracking Based on LLM**<br>
[X. Li](https://github.com/Xuchen-Li), [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), J. Zhang, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[the 3rd Workshop on Vision Datasets Understanding and DataCV Challenge in CVPR 2024](https://sites.google.com/view/vdu-cvpr24/) (Workshop in CCF-A Conference, Oral, Best Paper Honorable Mention)*<br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Large Language Model ğŸ“Œ Evaluation Technique<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2405.12139) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/DTLLM-VLT.bib)
[ğŸ“ƒ PDF](https://huuuuusy.github.io/files/DTLLM-VLT.pdf)
[ğŸª§ Poster](https://github.com/Xuchen-Lifiles/DTLLM-poster.pdf)
[ğŸ“¹ Slides](https://github.com/Xuchen-Lifiles/DTLLM-Slides.pdf)
[ğŸŒ Platform](http://videocube.aitestunion.com/)
[ğŸ”§ Toolkit](https://github.com/Xuchen-Li/DTLLM-VLT) 
[ğŸ’¾ Dataset](http://videocube.aitestunion.com/downloads)
[ğŸ† Award](https://huuuuusy.github.io/files/DTLLM-VLT-Award.pdf)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2025</div><img src='../../images/ICASSP25.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='ICASSP25'></span>

**Enhancing Vision-Language Tracking by Effectively Converting Textual Cues into Visual Cues**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Li](https://github.com/Xuchen-Li),  [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) <br>
*[IEEE International Conference on Acoustics, Speech, and Signal Processing](https://2025.ieeeicassp.org/) (CCF-B Conference, Poster)*<br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Multi-modal Learning ğŸ“Œ Grounding Model<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2024</div><img src='../../images/ICASSP24.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='ICASSP24'></span>

**Robust Single-particle Cryo-EM Image Denoising and Restoration**<br>
J. Zhang, T. Zhao, ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
*[IEEE International Conference on Acoustics, Speech, and Signal Processing](https://2024.ieeeicassp.org/) (CCF-B Conference, Poster)*<br>
ğŸ“Œ Medical Image Processing ğŸ“Œ AI4Science ğŸ“Œ Diffusion Model<br>
[ğŸ“ƒ Paper](https://ieeexplore.ieee.org/abstract/document/10447135) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/ICASSP24.bib)
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/ICASSP24.pdf)

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TCSVT 2024</div><img src='../../images/AWCV.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='AWCV'></span>

**Finger in Camera Speaks Everything: Unconstrained Air-Writing for Real-World**<br>
[M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi), [Y. Cai](https://teacher.bupt.edu.cn/caiyuanqiang/zh_CN/index.htm), ***<font color=DarkRed>Shiyu Hu</font>***, [Y. Zhao](https://callsys.github.io/zhaoyuzhong.github.io-main/), [W. Wang](https://people.ucas.ac.cn/~wqwang?language=en) <br>
*[IEEE Transactions on Circuits and Systems for Video Technology](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76) (CCF-B Journal)*<br>
ğŸ“Œ Air-writing Technique ğŸ“Œ Benchmark Construction ğŸ“Œ Human-machine Interaction<br>
[ğŸ“ƒ Paper](https://ieeexplore.ieee.org/document/10496279) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/AWCV100k.bib)
[ğŸ“ƒ PDF](https://huuuuusy.github.io/files/AWCV100k.pdf)
[ğŸ”§ Toolkit](https://github.com/wmeiqi/AWCV) 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PRCV 2024</div><img src='../../images/VSLLM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VSLLM'></span>

**VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing Projection Test**<br>
[M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), Y. Kang, [X. Li](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Hu</font>***, X. Chen, Y. kang, [W. Wang](https://people.ucas.ac.cn/~wqwang?language=en), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) <br>
*[Chinese Conference on Pattern Recognition and Computer Vision](https://www.prcv.cn) (CCF-C Conference)*<br>
ğŸ“Œ Psychological Assessment System ğŸ“Œ Gamified Assessment ğŸ“Œ AI4Science<br>
[ğŸ“ƒ Paper](https://link.springer.com/chapter/10.1007/978-981-97-8692-3_17) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/VSLLM.bib)
[ğŸ“ƒ PDF](https://huuuuusy.github.io/files/VSLLM.pdf)

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PRCV 2023</div><img src='../../images/PRCV23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='PRCV23'></span>

**A Hierarchical Theme Recognition Model for Sandplay Therapy**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[Chinese Conference on Pattern Recognition and Computer Vision](https://www.prcv2023.cn/2023prcv) (CCF-C Conference, Poster)*<br>
ğŸ“Œ Psychological Assessment System ğŸ“Œ Gamified Assessment ğŸ“Œ AI4Science<br>
[ğŸ“ƒ Paper](https://link.springer.com/chapter/10.1007/978-981-99-8462-6_20) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/PRCV23.bib) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/PRCV23.pdf)
[ğŸ”– Supplementary](https://huuuuusy.github.io/files/PRCV23-Supp.pdf)
[ğŸª§ Poster](https://huuuuusy.github.io/files/PRCV23-poster.pdf)

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Neurocomputing 2022</div><img src='../../images/Neu22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='Neu22'></span>

**Revisiting Instance Search: A New Benchmark Using Cycle Self-training**<br>
[Y. Zhang](https://wesleyzhang1991.github.io/), [C. Liu](https://scholar.google.com/citations?user=atOfOgMAAAAJ&hl=zh-CN&oi=sra), [W. Chen](https://scholar.google.com/citations?user=KWVlYaMAAAAJ&hl=zh-CN&oi=sra), [X. Xu](https://scholar.google.com/citations?user=nJc6BvgAAAAJ&hl=zh-CN&oi=sra), [F. Wang](https://scholar.google.com/citations?user=WCRGTHsAAAAJ), [H. Li](https://scholar.google.com/citations?user=pHN-QIwAAAAJ&hl=zh-CN&oi=sra), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
*[Neurocomputing](https://www.sciencedirect.com/journal/neurocomputing)  (CCF-C Journal)*<br>
ğŸ“Œ Video Instance Search ğŸ“Œ Benchmark Construction  ğŸ“Œ Data Mining<br>
[ğŸ“ƒ Paper](https://www.sciencedirect.com/science/article/abs/pii/S0925231222007445) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/Neu22.bib) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/Neu22.pdf) 
[ğŸŒ Project](https://github.com/Instance-Search/) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">å›¾å­¦å­¦æŠ¥ 2021</div><img src='../../images/VTT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VTT'></span>

**Visual Turing: The Next Development of Computer Vision in The View of Human-computer Gaming (è§†è§‰å›¾çµï¼šä»äººæœºå¯¹æŠ—çœ‹è®¡ç®—æœºè§†è§‰ä¸‹ä¸€æ­¥å‘å±•)**<br>
[K. Huang](https://people.ucas.ac.cn/~huangkaiqi), [X. Zhao](https://www.xinzhaoai.com/), [Q. Li](https://scholar.google.com/citations?user=7xmxBagAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***<br>
*[Journal of Graphics](http://www.txxb.com.cn/CN/2095-302X/home.shtml) (ã€Šå›¾å­¦å­¦æŠ¥ã€‹, CCF-C Chinese Journal)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Intelligent Evaluation Technique  ğŸ“Œ AI4Science<br>
[ğŸ“ƒ Paper](http://www.txxb.com.cn/CN/10.11996/JG.j.2095-302X.2021030339) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/VTT.bib) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/VTT.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ä¸­å›½å¿ƒç†å«ç”Ÿæ‚å¿— 2025</div><img src='../../images/IGBA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='IGBA'></span>

**A Review of Intelligent Psychological Assessment Based on Interactive Environment (åŸºäºäº¤äº’ç¯å¢ƒçš„æ™ºèƒ½åŒ–å¿ƒç†æµ‹è¯„)**<br>
[K. Huang](https://people.ucas.ac.cn/~huangkaiqi), Y. Kang, C. Yan, ***<font color=DarkRed>Shiyu Hu</font>***, [L. Wang](https://people.ucas.ac.cn/~wanglg), [T. Tao](https://people.ucas.ac.cn/~0072960), [W. Gao](https://people.ucas.ac.cn/~0000893) <br>
*[Chinese Mental Health Journal](http://xlwszz.tgcssci.com/) (ã€Šä¸­å›½å¿ƒç†å«ç”Ÿæ‚å¿—ã€‹, CSSCI Journal, Top Psychological Journal in China)*<br>
ğŸ“Œ Psychological Assessment System ğŸ“Œ Gamified Assessment ğŸ“Œ AI4Science<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CSAI 2023</div><img src='../../images/CSAI23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='CSAI23'></span>

**Rethinking Similar Object Interference in Single Object Tracking**<br>
[Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
*[International Conference on Computer Science and Artificial Intelligence](http://www.csai.org/) (EI Conference, **Oral**)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Similar Object Interference ğŸ“Œ Data Mining<br>
[ğŸ“ƒ Paper](https://dl.acm.org/doi/abs/10.1145/3638584.3638644) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/CSAI23.bib) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/CSAI23.pdf)

</div>
</div>


## Preprint

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/FIOVA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='FIOVA'></span>

**FIOVA: A Multi-Annotator Benchmark for Human-Aligned Video Captioning**<br>
***<font color=DarkRed>Shiyu Hu</font>***\*, [X. Li\*](https://github.com/Xuchen-Li), [X. Li](https://github.com/XuzhaoLi), J. Zhang, [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), [X. Zhao](https://www.xinzhaoai.com/), [K. Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319) (*Equal Contributions)<br>
ğŸ“Œ Large Vision-Language Models ğŸ“Œ Video Caption ğŸ“Œ Video Understanding<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2410.15270) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2410.15270) 
[ğŸŒ Project](https://huuuuusy.github.io/fiova/) 
<!-- [ğŸ—’ bibTex](https://huuuuusy.github.io/files/FIOVA.bib)  -->
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/SOE.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='SOE'></span>

**Students Rather Than Experts: A New AI for Education Pipeline to Model More Human-like and Personalised Early Adolescences**<br>
Y. Ma\*, ***<font color=DarkRed>Shiyu Hu</font>***\*, [X. Li](https://github.com/Xuchen-Li), [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), [S. Liu](https://faculty.ecnu.edu.cn/_s8/lsq/main.psp), [K. Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319)  (*Equal Contributions) <br> 
ğŸ“Œ AI4Education ğŸ“Œ LLMs ğŸ“Œ LLM-based Agent<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2410.15701) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/SOE.bib) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2410.15701) 
[ğŸŒ Project](https://marsgemini.github.io/SOE-LVSA/) 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/DTVLT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='DTVLT'></span>

**DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM**<br>
[X. Li](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Large Language Model ğŸ“Œ Evaluation Technique<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2410.02492) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/DTVLT.bib) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2410.02492) 
[ğŸŒ Project](http://videocube.aitestunion.com/) 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/VLT-MI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VLT-MI'></span>

**Visual Language Tracking with Multi-modal Interaction: A Robust Benchmark**<br>
[X. Li](https://github.com/Xuchen-Li),  ***<font color=DarkRed>Shiyu Hu</font>***, [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) <br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Multi-modal Interaction ğŸ“Œ Evaluation Technology<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2409.08887) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/VLT-MI.bib) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2409.08887) 
[ğŸŒ Project](http://videocube.aitestunion.com/) 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/VTT-ICLR.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VTT-ICLR'></span>

**Nearing or Surpassing: Overall Evaluation of Human-Machine Dynamic Vision Ability**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), [Y. Shan](https://scholar.google.com/citations?user=_nc83HsAAAAJ), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) <br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Intelligent Evaluation Technique ğŸ“Œ AI4Science<br>
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/VTT-ICLR.pdf)
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/VTT-ICLR.bib) 
</div>
</div>

<!-- ## Under Review

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">TCSVT 2024</div><img src='../../images/SOI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='SOI'></span>

**Target or Distractor? Rethinking Similar Object Interference in Single Object Tracking**<br>
[Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), [T. Yao](http://tingyao.deepfun.club/), [Y. Wang](https://scholar.google.com/citations?user=3nMDEBYAAAAJ), [L. Chen](https://sie.bit.edu.cn/szdw/jsml/ldjsyjsj/zgzcl/06c26b3ebaae4db981aaa388c660c8b5.htm), [X. Zhao](https://www.xinzhaoai.com/) <br>
*[IEEE Transactions on Circuits and Systems for Video Technology](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76) (CCF-B Journal, Under Review)*<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Similar Object Interference ğŸ“Œ Data Mining<br>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">CCF-A 2024</div><img src='../../images/ATCTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='ATCTrack'></span>

**ATCTrack: Leveraging Aligned Target-Context Cues for Robust Vision-Language Tracking**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Li](https://github.com/Xuchen-Li), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
Submitted to a CCF-A conference, under review<br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Multi-modal Alignment ğŸ“Œ Feature Awareness<br>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">CCF-A 2024</div><img src='../../images/MMAW.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MMAW'></span>

**Unconstrained Multimodal Air-Writing Benchmark: Writing by Moving Your Fingers in 3D**<br>
[M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), [X. Li](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Hu</font>***, [Y. Cai](https://teacher.bupt.edu.cn/caiyuanqiang/zh_CN/index.htm), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi), [W. Wang](https://people.ucas.ac.cn/~wqwang?language=en) <br>
Submitted to a CCF-A conference, under review<br>
ğŸ“Œ Air-writing Technique ğŸ“Œ Benchmark Construction ğŸ“Œ Human-machine Interaction<br>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">å¿ƒç†å­¦æŠ¥ 2024</div><img src='../../images/Sandplay.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='Sandplay'></span>

**Intelligent Psychological Assessment with Sandplay based on Evidence-Centered Design Theory (åŸºäºè¯æ®ä¸­å¿ƒè®¾è®¡ç†è®ºçš„æ™ºèƒ½å¿ƒç†æ²™ç›˜æµ‹è¯„ç³»ç»Ÿ)**<br>
Y. Ren, [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, Y. Kang, C. Yan, Y. Zeng, [L. Wang](https://people.ucas.ac.cn/~wanglg), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
*[Acta Psychologica Sinica](https://journal.psych.ac.cn/xlxb/CN/0439-755X/home.shtml) (ã€Šå¿ƒç†å­¦æŠ¥ã€‹, CSSCI Journal, Top-1 Psychological Journal in China, Under Review)*<br>
ğŸ“Œ Psychological Assessment System ğŸ“Œ Gamified Assessment ğŸ“Œ AI4Science<br>

</div>
</div> -->