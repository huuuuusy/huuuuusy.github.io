# ğŸ“ Publications

## Book


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Springer 2025</div><img src='../../images/SpringerBook.webp' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='SpringerBook'></span>

**Visual Object Tracking: An Evaluation Perspective**<br>
[X. Zhao](https://www.xinzhaoai.com/), ***<font color=DarkRed>Shiyu Hu</font>***,  [X. Yin](https://scce.ustb.edu.cn/shiziduiwu/jiaoshixinxi/2018-04-12/62.html)<br>
[Springer, Part of the book series: Advances in Computer Vision and Pattern Recognition (ACVPR)](https://www.springer.com/series/4205)<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Intelligent Evaluation Technology <br>
[ğŸ“ƒ Book](https://link.springer.com/book/9789819645572)

</div>
</div>


## Accept

### First Author / Corresponding Author
<!-- ä»£è¡¨ä½œæŒ‰ç…§å›ºå®šé¡ºåºæ’åˆ— -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2023</div><img src='../../images/GIT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='GIT'></span>

**Global Instance Tracking: Locating Target More Like Humans**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [L. Huang](https://github.com/huanglianghua), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
[IEEE Transactions on Pattern Analysis and Machine Intelligence](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34) (CCF-A Journal)<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Large-scale Benchmark Construction ğŸ“Œ Intelligent Evaluation Technology <br>
[ğŸ“ƒ Paper](https://ieeexplore.ieee.org/document/9720246/)
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/GIT.pdf)
[ğŸª§ Poster](https://huuuuusy.github.io/files/VALSE24Poster-364.pdf)
[ğŸŒ Platform](http://videocube.aitestunion.com/)
[ğŸ”§ Toolkit](https://github.com/huuuuusy/videocube-toolkit) 
[ğŸ’¾ Dataset](http://videocube.aitestunion.com/downloads)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV 2024</div><img src='../../images/SOTVerse.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='SOTVerse'></span>

**SOTVerse: A User-defined Task Space of Single Object Tracking**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
[International Journal of Computer Vision](https://www.springer.com/journal/11263) (CCF-A Journal)<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Dynamic Open Environment Construction ğŸ“Œ 3E Paradigm<br>
[ğŸ“ƒ Paper](https://link.springer.com/article/10.1007/s11263-023-01908-5)
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/SOTVerse.pdf)
[ğŸª§ Poster](https://huuuuusy.github.io/files/VALSE25Poster-G19.jpg)
[ğŸŒ Platform](http://metaverse.aitestunion.com/) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV 2024</div><img src='../../images/BioDrone.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='BioDrone'></span>

**BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision**<br>
[X. Zhao](https://www.xinzhaoai.com/), ***<font color=DarkRed>Shiyu Huâœ‰ï¸</font>***, [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), J. Zhang, Y. Hu, R. Liu, [H. Lin](https://www3.cs.stonybrook.edu/~hling/), [Y. Li](https://www.biostat.wisc.edu/~yli/), R. Li, K. Liu, [J. Li](http://yjsb.sinano.ac.cn/Doctor/info.aspx?itemid=920) <br>
[International Journal of Computer Vision](https://www.springer.com/journal/11263) (CCF-A Journal)<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Drone-based Tracking ğŸ“Œ Visual Robustness<br>
[ğŸ“ƒ Paper](https://link.springer.com/article/10.1007/s11263-023-01937-0)
[ğŸŒ Platform](http://biodrone.aitestunion.com/) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/BioDrone.pdf)
[ğŸ”§ Toolkit](https://github.com/huuuuusy/biodrone-toolkit-official) 
[ğŸ’¾ Dataset](http://biodrone.aitestunion.com/downloads) 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='../../images/MGIT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MGIT'></span>

**A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and causal Relationship**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [X. Li](https://github.com/Xuchen-Li), [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
[Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2023) (CCF-A Conference, Poster)<br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Long Video Understanding and Reasoning ğŸ“Œ Hierarchical Semantic Information Annotation<br>
[ğŸ“ƒ Paper](https://proceedings.nips.cc/paper_files/paper/2023/hash/4ea14e6090343523ddcd5d3ca449695f-Abstract-Datasets_and_Benchmarks.html) 
[ğŸ“ƒ PDF](https://huuuuusy.github.io/files/MGIT.pdf)
[ğŸª§ Poster](https://huuuuusy.github.io/files/MGIT-poster.pdf)
[ğŸ“¹ Slides](https://huuuuusy.github.io/files/MGIT-Slides.pdf)
[ğŸŒ Platform](http://videocube.aitestunion.com/)
[ğŸ”§ Toolkit](https://github.com/huuuuusy/videocube-toolkit) 
[ğŸ’¾ Dataset](http://videocube.aitestunion.com/downloads)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='../../images/ATCTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='ATCTrack'></span>

**ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking**<br>
[X. Feng\*](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***\*, [X. Li](https://github.com/Xuchen-Li), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) (*Equal Contributions) <br>
[International Conference on Computer Vision](https://iccv.thecvf.com/) (CCF-A Conference, **<font color=DarkRed>Highlight</font>**)<br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Multimodal Learning ğŸ“Œ Adaptive Prompts<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2507.19875)
[ğŸ“‘ PDF](https://arxiv.org/pdf/2507.19875)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2026</div><img src='../../images/MATrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MATrack'></span>

**MATrack: Efficient Multiscale Adaptive Tracker for Real-Time Nighttime UAV Operations**<br>
[X. Li\*](https://github.com/XuzhaoLi), [X. Li\*](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Huâœ‰ï¸</font>***<br>
[International Conference on Robotics and Automation](https://2026.ieee-icra.org/) (CAAI-A Conference)<br>
ğŸ“Œ Nighttime UAVs Tracking ğŸ“Œ Multiscale Adaptive Tracker ğŸ“Œ Visual Object Tracking <br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2510.21586)
[ğŸ“‘ PDF](https://arxiv.org/pdf/2510.21586)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICMR 2025</div><img src='../../images/DARTer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='DARTer'></span>

**DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking**<br>
[X. Li\*](https://github.com/XuzhaoLi), [X. Li\*](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Huâœ‰ï¸</font>***<br>
[International Conference on Multimedia Retrieval](https://www.icmr-2025.org/) (CCF-B Conference)<br>
ğŸ“Œ Nighttime UAVs Tracking ğŸ“Œ Dark Feature Blending ğŸ“Œ Dynamic Feature Activation <br>
[ğŸ“ƒ Paper](https://dl.acm.org/doi/abs/10.1145/3731715.3733473)
[ğŸ“‘ PDF](https://dl.acm.org/doi/pdf/10.1145/3731715.3733473)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ä¸­å›½å›¾è±¡å›¾å½¢å­¦æŠ¥ 2023</div><img src='../../images/Survey23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='JIG-survey'></span>

**Visual Intelligence Evaluation Techniques for Single Object Tracking: A Survey (å•ç›®æ ‡è·Ÿè¸ªä¸­çš„è§†è§‰æ™ºèƒ½è¯„ä¼°æŠ€æœ¯ç»¼è¿°)**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
[Journal of Images and Graphics](http://www.cjig.cn/jig/ch/index.aspx) (ã€Šä¸­å›½å›¾è±¡å›¾å½¢å­¦æŠ¥ã€‹, CCF-B Chinese Journal)<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Intelligent Evaluation Technique ğŸ“Œ AI4Science<br>
[ğŸ“ƒ Paper](http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?flag=2&file_no=202307100000002&journal_id=jig) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/JIG-survey.pdf) 

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IET-CVI 2025</div><img src='../../images/MSAD.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MSAD'></span>

**Improved SAR Aircraft Detection Algorithm Based on Visual State Space Models**<br>
Y. Wang, J. Zhang, [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), ***<font color=DarkRed>Shiyu Huâœ‰ï¸</font>***, B. Shen, Z. Hou, [W. Zhou](https://scholar.google.com/citations?user=r8x76hUAAAAJ)<br>
[IET Computer Vision](https://digital-library.theiet.org/journal/iet-cvi) (CCF-C Journal)<br>
ğŸ“Œ Synthetic Aperture Radar ğŸ“Œ State Space Models ğŸ“Œ Aircraft Object Detection <br>
<!-- [ğŸ“ƒ Paper](https://www.arxiv.org/abs/2505.00752) -->
<!-- [ğŸ“‘ PDF](https://www.arxiv.org/pdf/2505.00752) -->
</div>
</div>

### Collaborator (Arranged in Chronological Order)

<!-- åˆä½œè®ºæ–‡æŒ‰æ—¶é—´é¡ºåºæ’åˆ— -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2026</div><img src='../../images/CausalStep.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='CausalStep'></span>

**CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos**<br>
[X. Li\*](https://github.com/Xuchen-Li), [X. Li\*](https://github.com/XuzhaoLi), ***<font color=DarkRed>Shiyu Hu</font>***, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi), [W. Zhang](https://zwt233.github.io/)<br>
[Proceedings of the AAAI Conference on Artificial Intelligence](https://aaai.org/conference/aaai/aaai-26/) (CCF-A Conference, **<font color=DarkRed>Oral</font>**)<br>
ğŸ“Œ Video-based QA ğŸ“Œ Video Reasoning ğŸ“Œ Video Understanding <br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2507.16878)
[ğŸ“‘ PDF](https://arxiv.org/pdf/2507.16878)
[ğŸ“¹ Slides](https://huuuuusy.github.io/files/CausalStep-Slides.pdf)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2026</div><img src='../../images/VerifyBench.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VerifyBench'></span>

**VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains**<br>
[X. Li\*](https://github.com/XuzhaoLi), [X. Li\*](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Hu</font>***, [Y. Guo](https://openreview.net/profile?id=~yongzhen.gyz1), [W. Zhang](https://zwt233.github.io/)<br>
[Proceedings of the AAAI Conference on Artificial Intelligence](https://aaai.org/conference/aaai/aaai-26/) (CCF-A Conference, **<font color=DarkRed>Oral</font>**)<br>
ğŸ“Œ Verifable Reward ğŸ“Œ Reinforcement Learning <br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2507.09884)
[ğŸ“‘ PDF](https://arxiv.org/pdf/2507.09884)
[ğŸ“¹ Slides](https://huuuuusy.github.io/files/VerifyBench-Slides.pdf)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2026</div><img src='../../images/NarrLV.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='NarrLV'></span>

**NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), H. Yu, [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, J. Chen, C. Zhu, J. Wu, [X. Chu](https://cxxgtxy.github.io/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
ğŸ“Œ Visual Understanding ğŸ“Œ Video Generation ğŸ“Œ Evaluation Technique<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2507.11245) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2507.11245) 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2025</div><img src='../../images/CSTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='CSTrack'></span>

**CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Li](https://github.com/Xuchen-Li), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
[International Conference on Machine Learning](https://icml.cc/) (CCF-A Conference, Poster)<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Multi-modal Learning <br>
[ğŸ“ƒ Paper](https://openreview.net/forum?id=JZIJxr9KsO)
[ğŸ“‘ PDF](https://openreview.net/pdf?id=JZIJxr9KsO5)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2025</div><img src='../../images/ICASSP25.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='ICASSP25'></span>

**Enhancing Vision-Language Tracking by Effectively Converting Textual Cues into Visual Cues**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Li](https://github.com/Xuchen-Li),  [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) <br>
[IEEE International Conference on Acoustics, Speech, and Signal Processing](https://2025.ieeeicassp.org/) (CCF-B Conference, Poster)<br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Multi-modal Learning ğŸ“Œ Grounding Model<br>
[ğŸ“ƒ Paper](https://ieeexplore.ieee.org/document/10888064) 
[ğŸ“ƒ PDF](https://arxiv.org/pdf/2412.19648)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">C&E:AI 2025</div><img src='../../images/CEAI-adaptive.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='CEAI-adaptive'></span>

**Artificial Intelligence-Enabled Adaptive Learning Platforms: A Review**<br>
L. Tan, ***<font color=DarkRed>Shiyu Hu</font>***, [Darren J. Yeo](https://dr.ntu.edu.sg/cris/rp/rp01327), [KH Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319) <br>
[Computers & Education: Artificial Intelligence](https://www.sciencedirect.com/journal/computers-and-education-artificial-intelligence)<br>
ğŸ“Œ Adaptive Learning Platforms ğŸ“Œ AI for Education ğŸ“Œ Educational Technology<br>
[ğŸ“ƒ Paper](https://www.sciencedirect.com/science/article/pii/S2666920X25000694) 
[ğŸ“‘ PDF](https://www.sciencedirect.com/science/article/pii/S2666920X25000694/pdfft?md5=78d8390b2042b0b007698f2e3db4fe76&pid=1-s2.0-S2666920X25000694-main.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Mathematics 2025</div><img src='../../images/Mathematics-AGS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='Mathematics-AGS'></span>

**A Comprehensive Review on Automated Grading Systems in STEM Using AI Techniques**<br>
L. Tan, ***<font color=DarkRed>Shiyu Hu</font>***, [Darren J. Yeo](https://dr.ntu.edu.sg/cris/rp/rp01327), [KH Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319) <br>
[Mathematics](https://www.sciencedirect.com/journal/computers-and-education-artificial-intelligence)<br>
ğŸ“Œ Automated Grading Systems ğŸ“Œ AI for Education ğŸ“Œ Educational Technology<br>
[ğŸ“ƒ Paper](https://www.mdpi.com/2227-7390/13/17/2828) 
<!-- [ğŸ“‘ PDF](https://www.sciencedirect.com/science/article/pii/S2666920X25000694/pdfft?md5=78d8390b2042b0b007698f2e3db4fe76&pid=1-s2.0-S2666920X25000694-main.pdf) -->

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Innovation and Emerging Technologies 2025</div><img src='../../images/IET-AI4Edu.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='IET-AI4Edu'></span>

**Trustworthy AI in education: Framework, cases, and governance strategies**<br>
Y. Ma, X. Li, ***<font color=DarkRed>Shiyu Hu</font>***, [S. Liu](https://faculty.ecnu.edu.cn/_s8/lsq/main.psp), [KH Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319)  <br>
[Innovation and Emerging Technologies](https://www.worldscientific.com/worldscinet/iet?cmpid=6350db6df59e5b0001f79b4b)<br>
ğŸ“Œ Trustworthy Artificial Intelligence ğŸ“Œ Educational Governance ğŸ“Œ Algorithmic Fairness;<br>
[ğŸ“ƒ Paper](https://www.worldscientific.com/doi/abs/10.1142/S2737599425500264) 
<!-- [ğŸ“‘ PDF](https://www.sciencedirect.com/science/article/pii/S2666920X25000694/pdfft?md5=78d8390b2042b0b007698f2e3db4fe76&pid=1-s2.0-S2666920X25000694-main.pdf) -->

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ä¸­å›½å¿ƒç†å«ç”Ÿæ‚å¿— 2025</div><img src='../../images/IGBA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='IGBA'></span>

**A Review of Intelligent Psychological Assessment Based on Interactive Environment (åŸºäºäº¤äº’ç¯å¢ƒçš„æ™ºèƒ½åŒ–å¿ƒç†æµ‹è¯„)**<br>
[K. Huang](https://people.ucas.ac.cn/~huangkaiqi), Y. Kang, C. Yan, ***<font color=DarkRed>Shiyu Hu</font>***, [L. Wang](https://people.ucas.ac.cn/~wanglg), [T. Tao](https://people.ucas.ac.cn/~0072960), [W. Gao](https://people.ucas.ac.cn/~0000893) <br>
[Chinese Mental Health Journal](http://xlwszz.tgcssci.com/) (ã€Šä¸­å›½å¿ƒç†å«ç”Ÿæ‚å¿—ã€‹, CSSCI Journal, Top Psychological Journal in China)<br>
ğŸ“Œ Psychological Assessment System ğŸ“Œ Gamified Assessment ğŸ“Œ AI4Science<br>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='../../images/CPDTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='CPDTrack'></span>

**Beyond Accuracy: Tracking more like Human via Visual Search**<br>
[D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [X. Li](https://github.com/Xuchen-Li), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
[Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2024) (CCF-A Conference, Poster)<br>
ğŸ“Œ  Visual Object Tracking ğŸ“Œ Visual Search Mechanism ğŸ“Œ Visual Turing Test<br>
[ğŸ“ƒ Paper](https://proceedings.neurips.cc/paper_files/paper/2024/hash/050f8591be3874b52fdac4e1060eeb29-Abstract-Conference.htmlO)
[ğŸ“‘ PDF](https://proceedings.neurips.cc/paper_files/paper/2024/file/050f8591be3874b52fdac4e1060eeb29-Paper-Conference.pdf)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='../../images/MemVLT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MemVLT'></span>

**MemVLT: Vision-Language Tracking with Adaptive Memory-based Prompts**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [X. Li](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Hu</font>***, [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
[Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2024) (CCF-A Conference, Poster)<br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Human-like Memory Modeling ğŸ“Œ Adaptive Prompts<br>
[ğŸ“ƒ Paper](https://neurips.cc/virtual/2024/poster/94643)
[ğŸ“‘ PDF](https://proceedings.neurips.cc/paper_files/paper/2024/file/1af3e0bf5905e33789979f666c31192d-Paper-Conference.pdf)

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2024</div><img src='../../images/ICASSP24.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='ICASSP24'></span>

**Robust Single-particle Cryo-EM Image Denoising and Restoration**<br>
J. Zhang, T. Zhao, ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
[IEEE International Conference on Acoustics, Speech, and Signal Processing](https://2024.ieeeicassp.org/) (CCF-B Conference, Poster)<br>
ğŸ“Œ Medical Image Processing ğŸ“Œ AI4Science ğŸ“Œ Diffusion Model<br>
[ğŸ“ƒ Paper](https://ieeexplore.ieee.org/abstract/document/10447135) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/ICASSP24.pdf)

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TCSVT 2024</div><img src='../../images/AWCV.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='AWCV'></span>

**Finger in Camera Speaks Everything: Unconstrained Air-Writing for Real-World**<br>
[M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi), [Y. Cai](https://teacher.bupt.edu.cn/caiyuanqiang/zh_CN/index.htm), ***<font color=DarkRed>Shiyu Hu</font>***, [Y. Zhao](https://callsys.github.io/zhaoyuzhong.github.io-main/), [W. Wang](https://people.ucas.ac.cn/~wqwang?language=en) <br>
[IEEE Transactions on Circuits and Systems for Video Technology](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76) (CCF-B Journal)<br>
ğŸ“Œ Air-writing Technique ğŸ“Œ Benchmark Construction ğŸ“Œ Human-machine Interaction<br>
[ğŸ“ƒ Paper](https://ieeexplore.ieee.org/document/10496279) 
[ğŸ“ƒ PDF](https://huuuuusy.github.io/files/AWCV100k.pdf)
[ğŸ”§ Toolkit](https://github.com/wmeiqi/AWCV) 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PRCV 2024</div><img src='../../images/VSLLM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VSLLM'></span>

**VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing Projection Test**<br>
[M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), Y. Kang, [X. Li](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Hu</font>***, X. Chen, Y. kang, [W. Wang](https://people.ucas.ac.cn/~wqwang?language=en), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) <br>
[Chinese Conference on Pattern Recognition and Computer Vision](https://www.prcv.cn) (CCF-C Conference)<br>
ğŸ“Œ Psychological Assessment System ğŸ“Œ Gamified Assessment ğŸ“Œ AI4Science<br>
[ğŸ“ƒ Paper](https://link.springer.com/chapter/10.1007/978-981-97-8692-3_17) 
[ğŸ“ƒ PDF](https://huuuuusy.github.io/files/VSLLM.pdf)

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PRCV 2023</div><img src='../../images/PRCV23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='PRCV23'></span>

**A Hierarchical Theme Recognition Model for Sandplay Therapy**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
[Chinese Conference on Pattern Recognition and Computer Vision](https://www.prcv2023.cn/2023prcv) (CCF-C Conference, Poster)<br>
ğŸ“Œ Psychological Assessment System ğŸ“Œ Gamified Assessment ğŸ“Œ AI4Science<br>
[ğŸ“ƒ Paper](https://link.springer.com/chapter/10.1007/978-981-99-8462-6_20) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/PRCV23.pdf)
[ğŸ”– Supplementary](https://huuuuusy.github.io/files/PRCV23-Supp.pdf)
[ğŸª§ Poster](https://huuuuusy.github.io/files/PRCV23-poster.pdf)

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CSAI 2023</div><img src='../../images/CSAI23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='CSAI23'></span>

**Rethinking Similar Object Interference in Single Object Tracking**<br>
[Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
[International Conference on Computer Science and Artificial Intelligence](http://www.csai.org/) (EI Conference, **<font color=DarkRed>Oral</font>**)<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Similar Object Interference ğŸ“Œ Data Mining<br>
[ğŸ“ƒ Paper](https://dl.acm.org/doi/abs/10.1145/3638584.3638644) 
[ğŸ—’ bibTex](https://huuuuusy.github.io/files/CSAI23.bib) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/CSAI23.pdf)

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Neurocomputing 2022</div><img src='../../images/Neu22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='Neu22'></span>

**Revisiting Instance Search: A New Benchmark Using Cycle Self-training**<br>
[Y. Zhang](https://wesleyzhang1991.github.io/), [C. Liu](https://scholar.google.com/citations?user=atOfOgMAAAAJ&hl=zh-CN&oi=sra), [W. Chen](https://scholar.google.com/citations?user=KWVlYaMAAAAJ&hl=zh-CN&oi=sra), [X. Xu](https://scholar.google.com/citations?user=nJc6BvgAAAAJ&hl=zh-CN&oi=sra), [F. Wang](https://scholar.google.com/citations?user=WCRGTHsAAAAJ), [H. Li](https://scholar.google.com/citations?user=pHN-QIwAAAAJ&hl=zh-CN&oi=sra), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
[Neurocomputing](https://www.sciencedirect.com/journal/neurocomputing)  (CCF-C Journal)<br>
ğŸ“Œ Video Instance Search ğŸ“Œ Benchmark Construction  ğŸ“Œ Data Mining<br>
[ğŸ“ƒ Paper](https://www.sciencedirect.com/science/article/abs/pii/S0925231222007445) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/Neu22.pdf) 
[ğŸŒ Project](https://github.com/Instance-Search/) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">å›¾å­¦å­¦æŠ¥ 2021</div><img src='../../images/VTT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VTT'></span>

**Visual Turing: The Next Development of Computer Vision in The View of Human-computer Gaming (è§†è§‰å›¾çµï¼šä»äººæœºå¯¹æŠ—çœ‹è®¡ç®—æœºè§†è§‰ä¸‹ä¸€æ­¥å‘å±•)**<br>
[K. Huang](https://people.ucas.ac.cn/~huangkaiqi), [X. Zhao](https://www.xinzhaoai.com/), [Q. Li](https://scholar.google.com/citations?user=7xmxBagAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***<br>
[Journal of Graphics](http://www.txxb.com.cn/CN/2095-302X/home.shtml) (ã€Šå›¾å­¦å­¦æŠ¥ã€‹, CCF-C Chinese Journal)<br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Intelligent Evaluation Technique  ğŸ“Œ AI4Science<br>
[ğŸ“ƒ Paper](http://www.txxb.com.cn/CN/10.11996/JG.j.2095-302X.2021030339) 
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/VTT.pdf)

</div>
</div>




## Workshop


- ``AAAIW 2026``
**Learning to Be Taught: A Structured SOEI Framework for Modeling and Evaluating Personality-Aligned Virtual Student Agents**, 
Y. Ma\*, ***<font color=DarkRed>Shiyu Hu</font>***\*, [X. Li](https://github.com/Xuchen-Li), [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), Y. Chen, [S. Liu](https://faculty.ecnu.edu.cn/_s8/lsq/main.psp), [KH Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319)  (*Equal Contributions),
[the AI for Education Workshop in the 40th Annual AAAI Conference on Artificial Intelligence](https://ai4ed.cc/workshops/aaai2026) (Workshop in CCF-A Conference),
[ğŸ“¹ Slides](https://huuuuusy.github.io/files/AAAI26-AI4Edu-SOEI-Slides.pdf)

- ``AAAIW 2026``
**Redefining Educational Simulation: EduVerse as a User-Defined and Developmental Multi-Agent Simulation Space**, 
Y. Ma\*, ***<font color=DarkRed>Shiyu Hu</font>***\*, B. Zhu, [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), Y. Kang, [S. Liu](https://faculty.ecnu.edu.cn/_s8/lsq/main.psp), [KH Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319)  (*Equal Contributions), 
[the AI for Education Workshop in the 40th Annual AAAI Conference on Artificial Intelligence](https://ai4ed.cc/workshops/aaai2026) (Workshop in CCF-A Conference),
[ğŸ“¹ Slides](https://huuuuusy.github.io/files/AAAI26-AI4Edu-EduVerse-Slides.pdf)


- ``AAAIW 2026``
**From Objective to Subjective: A Benchmark for Virtual Student Abilities**, 
B. Zhu\*, ***<font color=DarkRed>Shiyu Hu</font>***\*, Y. Ma, Y. Zhang, [KH Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319) (*Equal Contributions), 
[the AI for Education Workshop in the 40th Annual AAAI Conference on Artificial Intelligence](https://ai4ed.cc/workshops/aaai2026) (Workshop in CCF-A Conference),
[ğŸ“¹ Slides](https://huuuuusy.github.io/files/AAAI26-AI4Edu-EduPersona-Slides.pdf)


- ``CVPRW 2024``
**Diverse Text Generation for Visual Language Tracking Based on LLM**, 
[X. Li](https://github.com/Xuchen-Li), [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), J. Zhang, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi), 
[the 3rd Workshop on Vision Datasets Understanding and DataCV Challenge in CVPR 2024](https://sites.google.com/view/vdu-cvpr24/) (Workshop in CCF-A Conference, **<font color=DarkRed>Oral, Best Paper Honorable Mention</font>**), 
[ğŸ“ƒ Paper](https://arxiv.org/abs/2405.12139) 
[ğŸ“ƒ PDF](https://huuuuusy.github.io/files/DTLLM-VLT.pdf)
[ğŸª§ Poster](https://github.com/Xuchen-Lifiles/DTLLM-poster.pdf)
[ğŸ“¹ Slides](https://github.com/Xuchen-Lifiles/DTLLM-Slides.pdf)
[ğŸŒ Platform](http://videocube.aitestunion.com/)
[ğŸ”§ Toolkit](https://github.com/Xuchen-Li/DTLLM-VLT) 
[ğŸ’¾ Dataset](http://videocube.aitestunion.com/downloads)
[ğŸ† Award](https://huuuuusy.github.io/files/DTLLM-VLT-Award.pdf)




## Preprint

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/FIOVA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='FIOVA'></span>

**FIOVA: A Multi-Annotator Benchmark for Human-Aligned Video Captioning**<br>
***<font color=DarkRed>Shiyu Hu</font>***\*, [X. Li\*](https://github.com/Xuchen-Li), [X. Li](https://github.com/XuzhaoLi), J. Zhang, [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), [X. Zhao](https://www.xinzhaoai.com/), [KH Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319) (*Equal Contributions)<br>
ğŸ“Œ Large Vision-Language Models ğŸ“Œ Video Caption ğŸ“Œ Video Understanding<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2410.15270) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2410.15270) 
[ğŸŒ Project](https://huuuuusy.github.io/fiova/) 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/SOEI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='SOEI'></span>

**When LLMs Learn to be Students: The SOEI Framework for Modeling and Evaluating Virtual Student Agents in Educational Interaction**<br>
Y. Ma\*, ***<font color=DarkRed>Shiyu Hu</font>***\*, [X. Li](https://github.com/Xuchen-Li), [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), Y. Chen, [S. Liu](https://faculty.ecnu.edu.cn/_s8/lsq/main.psp), [KH Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319)  (*Equal Contributions) <br> 
ğŸ“Œ AI4Education ğŸ“Œ LLMs ğŸ“Œ LLM-based Agent<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2410.15701) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2410.15701) 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/EduVerse.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='EduVerse'></span>

**EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario**<br>
Y. Ma\*, ***<font color=DarkRed>Shiyu Hu</font>***\*, B. Zhu, [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), Y. Kang, [S. Liu](https://faculty.ecnu.edu.cn/_s8/lsq/main.psp), [KH Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319)  (*Equal Contributions) <br> 
ğŸ“Œ AI4Education ğŸ“Œ LLMs ğŸ“Œ LLM-based Agent<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2510.05650) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2510.05650) 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/EduPersona.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='EduPersona'></span>

**EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents**<br>
B. Zhu\*, ***<font color=DarkRed>Shiyu Hu</font>***\*, Y. Ma, Y. Zhang, [KH Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319)  (*Equal Contributions) <br> 
ğŸ“Œ AI4Education ğŸ“Œ LLMs ğŸ“Œ LLM-based Agent<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2510.04648) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2510.04648) 
</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/SOI-V2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='SOI-V2'></span>

**SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking**<br>
[Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ)\*, ***<font color=DarkRed>Shiyu Hu</font>***\*, S. Jia, P. Xu, H. Ma, Y. Ma, J. Zhang, [X. Lu](https://automation.seu.edu.cn/lxb/list.htm), [X. Zhao](https://www.xinzhaoai.com/) (*Equal Contributions) <br> 
ğŸ“Œ Visual Object Tracking ğŸ“Œ Similar Object Interference ğŸ“Œ Multimodal Learning<br>
[ğŸ“ƒ Paper](https://www.arxiv.org/abs/2508.09524) 
[ğŸ“‘ PDF](https://www.arxiv.org/pdf/2508.09524) 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/VLTVerse.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VLTVerse'></span>

**How Texts Help? A Fine-grained Evaluation to Reveal the Role of Language in Vision-Language Tracking**<br>
[X. Li](https://github.com/Xuchen-Li)\*, ***<font color=DarkRed>Shiyu Hu</font>***\*, [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) (*Equal Contributions) <br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Multimodal Learning ğŸ“Œ Evaluation Technique<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2411.15600) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2411.15600) 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/DTVLT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='DTVLT'></span>

**DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM**<br>
[X. Li](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Large Language Model ğŸ“Œ Evaluation Technique<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2410.02492) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2410.02492) 
[ğŸŒ Project](http://videocube.aitestunion.com/) 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/VLT-MI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VLT-MI'></span>

**Visual Language Tracking with Multi-modal Interaction: A Robust Benchmark**<br>
[X. Li](https://github.com/Xuchen-Li),  ***<font color=DarkRed>Shiyu Hu</font>***, [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) <br>
ğŸ“Œ Visual Language Tracking ğŸ“Œ Multi-modal Interaction ğŸ“Œ Evaluation Technology<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2409.08887) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2409.08887) 
[ğŸŒ Project](http://videocube.aitestunion.com/) 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/TBDQ.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='TBDQ'></span>

**Tracking by Detection and Query: An Efficient End-to-End Framework for Multi-Object Tracking**<br>
S. Jia, ***<font color=DarkRed>Shiyu Hu</font>***, Y. Cao, F. Yang, X. Lu, [X. Lu](https://automation.seu.edu.cn/lxb/list.htm) <br> 
ğŸ“Œ Multi-object Tracking ğŸ“Œ Tracking by Detection ğŸ“Œ Tracking by Query<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2411.06197) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2411.06197) 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/EARL.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='EARL'></span>

**Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning**<br>
[X. Li\*](https://github.com/Xuchen-Li), [X. Li\*](https://github.com/XuzhaoLi), ***<font color=DarkRed>Shiyu Hu</font>***, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
ğŸ“Œ Video Large Language Models ğŸ“Œ Video Reasoning ğŸ“Œ Video Understanding <br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2510.15440)
[ğŸ“‘ PDF](https://arxiv.org/pdf/2510.15440)
</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/RGRL.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='RGRL'></span>

**Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning**<br>
[X. Li\*](https://github.com/Xuchen-Li), [X. Li\*](https://github.com/XuzhaoLi), [J. Gao](https://scholar.google.com/citations?user=0LzbaZcAAAAJ&hl=en), [R. Pi](https://scholar.google.com/citations?user=XUq0HwcAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [W. Zhang](https://zwt233.github.io/)<br>
ğŸ“Œ Thinking-with-Image ğŸ“Œ Vision-Language Models ğŸ“Œ Pixel Reasoning<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2510.01681) 
[ğŸ“‘ PDF](https://arxiv.org/pdf/2510.01681) 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/ViEBench.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='ViEBench'></span>

**Beyond Accuracy: Evaluating Grounded Visual Evidence in Thinking with Images**<br>
[X. Li\*](https://github.com/Xuchen-Li), [X. Li\*](https://github.com/XuzhaoLi), [R. Pi](https://scholar.google.com/citations?user=XUq0HwcAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [J. Zhao](https://scholar.google.com/citations?user=n6zuurcAAAAJ),[J. Gao](https://scholar.google.com/citations?user=0LzbaZcAAAAJ&hl=en)<br>
ğŸ“Œ Thinking-with-Image ğŸ“Œ Vision-Language Models ğŸ“Œ Agentic Models<br>
[ğŸ“ƒ Paper](https://arxiv.org/abs/2601.11633) 
[ğŸ“‘ PDF](https://arxiv.org/abs/2601.11633) 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/VTT-ICLR.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VTT-ICLR'></span>

**Nearing or Surpassing: Overall Evaluation of Human-Machine Dynamic Vision Ability**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), [Y. Shan](https://scholar.google.com/citations?user=_nc83HsAAAAJ), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) <br>
ğŸ“Œ Visual Object Tracking ğŸ“Œ Intelligent Evaluation Technique ğŸ“Œ AI4Science<br>
[ğŸ“‘ PDF](https://huuuuusy.github.io/files/VTT-ICLR.pdf)
</div>
</div>

