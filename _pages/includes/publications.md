# 📝 Publications

<!-- ## Paper Summary

### Journal

- **TPAMI**: IEEE Transactions on Pattern Analysis and Machine Intelligence (CCF-A Journal, Top-1 journal in computer vision, IF=20.8). ***<font color=DarkRed>Acceptance×1 (first author×1)</font>***
- **IJCV**: International Journal of Computer Vision (CCF-A Journal, Top-2 journal in computer vision, IF=11.6). ***<font color=DarkRed>Acceptance×2 (first author×1, corresponding-author×1)</font>***
- **TCSVT**: IEEE Transactions on Circuits and Systems for Video Technology (CCF-B Journal, IF=8.3). ***<font color=DarkRed>Acceptance×1, under review×1</font>***
- **JIG**: Journal of Images and Graphics (《中国图象图形学报》, CCF-B Chinese Journal). ***<font color=DarkRed>Acceptance×1 (first author×1)</font>***
- **JOG**: Journal of Graphics (《图学学报》, CCF-C Chinese Journal). ***<font color=DarkRed>Acceptance×1</font>***
- **Neu**: Neurocomputing (CCF-C Journal, IF=5.5). ***<font color=DarkRed>Acceptance×1</font>***
- **CMHJ**: Chinese Mental Health Journal (《中国心理卫生杂志》, CSSCI Journal, Top Psychological Journal in China) ***<font color=DarkRed>Acceptance×1</font>***
- **APS**: Acta Psychologica Sinica (《心理学报》, CSSCI Journal, Top-1 Psychological Journal in China). ***<font color=DarkRed>Under review×1</font>***

### Conference

- **NeurIPS**: Conference on Neural Information Processing Systems (CCF-A Conference). ***<font color=DarkRed>Acceptance×3 (first author×1)</font>***
- **ICLR**: International Conference on Learning Representations (CAAI-A Conference). ***<font color=DarkRed>Under review×3 (first author×2)</font>***
- **AAAI**: Annual AAAI Conference on Artificial Intelligence (CCF-A Conference). ***<font color=DarkRed>Under review×1</font>***
- **NeurIPSW**: Workshop in Conference on Neural Information Processing Systems (CCF-A Conference workshop). ***<font color=DarkRed>Under review×1</font>***
- **CVPRW**: Workshop in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CCF-A Conference workshop). ***<font color=DarkRed>Acceptance×1 (oral & best paper honorable mention×1)</font>***
- **ICASSP**: IEEE International Conference on Acoustics, Speech, and Signal Processing (CCF-B Conference). ***<font color=DarkRed>Acceptance×1, under review×1</font>***
- **PRCV**: Chinese Conference on Pattern Recognition and Computer Vision (CCF-C Conference). ***<font color=DarkRed>Acceptance×2</font>***
- **CSAI**: International Conference on Computer Science and Artificial Intelligence (EI Conference). ***<font color=DarkRed>Acceptance×1 (oral×1)</font>*** -->

## Book


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Springer 2025</div><img src='../../images/SpringerBook.webp' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='SpringerBook'></span>

**Visual Object Tracking: An Evaluation Perspective**<br>
[X. Zhao](https://www.xinzhaoai.com/), ***<font color=DarkRed>Shiyu Hu</font>***,  [X. Yin](https://scce.ustb.edu.cn/shiziduiwu/jiaoshixinxi/2018-04-12/62.html)<br>
*[Springer, Part of the book series: Advances in Computer Vision and Pattern Recognition (ACVPR)](https://www.springer.com/series/4205)*<br>
📌 Visual Object Tracking 📌 Intelligent Evaluation Technology <br>
[📃 Book](https://link.springer.com/book/9789819645572)
<!-- [🗒 bibTex](https://huuuuusy.github.io/files/GIT.bib) -->
<!-- [📑 PDF](https://huuuuusy.github.io/files/GIT.pdf) -->

</div>
</div>


## Acceptance

<!-- 前5篇代表作按照固定顺序排列 -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2023</div><img src='../../images/GIT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='GIT'></span>

**Global Instance Tracking: Locating Target More Like Humans**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [L. Huang](https://github.com/huanglianghua), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[IEEE Transactions on Pattern Analysis and Machine Intelligence](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34) (CCF-A Journal)*<br>
📌 Visual Object Tracking 📌 Large-scale Benchmark Construction 📌 Intelligent Evaluation Technology <br>
[📃 Paper](https://ieeexplore.ieee.org/document/9720246/)
[📑 PDF](https://huuuuusy.github.io/files/GIT.pdf)
[🪧 Poster](https://huuuuusy.github.io/files/VALSE24Poster-364.pdf)
[🌐 Platform](http://videocube.aitestunion.com/)
[🔧 Toolkit](https://github.com/huuuuusy/videocube-toolkit) 
[💾 Dataset](http://videocube.aitestunion.com/downloads)
<!-- [🗒 bibTex](https://huuuuusy.github.io/files/GIT.bib) -->

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV 2024</div><img src='../../images/SOTVerse.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='SOTVerse'></span>

**SOTVerse: A User-defined Task Space of Single Object Tracking**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[International Journal of Computer Vision](https://www.springer.com/journal/11263) (CCF-A Journal)*<br>
📌 Visual Object Tracking 📌 Dynamic Open Environment Construction 📌 3E Paradigm<br>
[📃 Paper](https://link.springer.com/article/10.1007/s11263-023-01908-5)
[📑 PDF](https://huuuuusy.github.io/files/SOTVerse.pdf)
[🌐 Platform](http://metaverse.aitestunion.com/) 
<!-- [🗒 bibTex](https://huuuuusy.github.io/files/SOTVerse.bib) -->

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV 2024</div><img src='../../images/BioDrone.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='BioDrone'></span>

**BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision**<br>
[X. Zhao](https://www.xinzhaoai.com/), ***<font color=DarkRed>Shiyu Hu✉️</font>***, [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), J. Zhang, Y. Hu, R. Liu, [H. Lin](https://www3.cs.stonybrook.edu/~hling/), [Y. Li](https://www.biostat.wisc.edu/~yli/), R. Li, K. Liu, [J. Li](http://yjsb.sinano.ac.cn/Doctor/info.aspx?itemid=920) <br>
*[International Journal of Computer Vision](https://www.springer.com/journal/11263) (CCF-A Journal)*<br>
📌 Visual Object Tracking 📌 Drone-based Tracking 📌 Visual Robustness<br>
[📃 Paper](https://link.springer.com/article/10.1007/s11263-023-01937-0)
[🌐 Platform](http://biodrone.aitestunion.com/) 
[📑 PDF](https://huuuuusy.github.io/files/BioDrone.pdf)
[🔧 Toolkit](https://github.com/huuuuusy/biodrone-toolkit-official) 
[💾 Dataset](http://biodrone.aitestunion.com/downloads) 
<!-- [🗒 bibTex](https://huuuuusy.github.io/files/BioDrone.bib)  -->
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='../../images/MGIT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MGIT'></span>

**A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and causal Relationship**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [X. Li](https://github.com/Xuchen-Li), [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2023) (CCF-A Conference, Poster)*<br>
📌 Visual Language Tracking 📌 Long Video Understanding and Reasoning 📌 Hierarchical Semantic Information Annotation<br>
[📃 Paper](https://proceedings.nips.cc/paper_files/paper/2023/hash/4ea14e6090343523ddcd5d3ca449695f-Abstract-Datasets_and_Benchmarks.html) 
[📃 PDF](https://huuuuusy.github.io/files/MGIT.pdf)
[🪧 Poster](https://huuuuusy.github.io/files/MGIT-poster.pdf)
[📹 Slides](https://huuuuusy.github.io/files/MGIT-Slides.pdf)
[🌐 Platform](http://videocube.aitestunion.com/)
[🔧 Toolkit](https://github.com/huuuuusy/videocube-toolkit) 
[💾 Dataset](http://videocube.aitestunion.com/downloads)
<!-- [🗒 bibTex](https://huuuuusy.github.io/files/MGIT.bib) -->
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">中国图象图形学报 2023</div><img src='../../images/Survey23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='JIG-survey'></span>

**Visual Intelligence Evaluation Techniques for Single Object Tracking: A Survey (单目标跟踪中的视觉智能评估技术综述)**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[Journal of Images and Graphics](http://www.cjig.cn/jig/ch/index.aspx) (《中国图象图形学报》, CCF-B Chinese Journal)*<br>
📌 Visual Object Tracking 📌 Intelligent Evaluation Technique 📌 AI4Science<br>
[📃 Paper](http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?flag=2&file_no=202307100000002&journal_id=jig) 
[📑 PDF](https://huuuuusy.github.io/files/JIG-survey.pdf) 

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICMR 2025</div><img src='../../images/DARTer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='DARTer'></span>

**DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking**<br>
[X. Li\*](https://github.com/XuzhaoLi), [X. Li\*](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Hu✉️</font>***, 
*[International Conference on Multimedia Retrieval](https://www.icmr-2025.org/) (CCF-B Conference)*<br>
📌 Nighttime UAVs Tracking 📌 Dark Feature Blending 📌 Dynamic Feature Activation <br>
[📃 Paper](https://www.arxiv.org/abs/2505.00752)
[📑 PDF](https://www.arxiv.org/pdf/2505.00752)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IET-CVI 2025</div><img src='../../images/MSAD.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MSAD'></span>

**Improved SAR Aircraft Detection Algorithm Based on Visual State Space Models**<br>
Y. Wang, J. Zhang, [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), ***<font color=DarkRed>Shiyu Hu✉️</font>***, B. Shen, Z. Hou, [W. Zhou](https://scholar.google.com/citations?user=r8x76hUAAAAJ). 
*[IET Computer Vision](https://digital-library.theiet.org/journal/iet-cvi) (CCF-C Journal)*<br>
📌 Synthetic Aperture Radar 📌 State Space Models 📌 Aircraft Object Detection <br>
<!-- [📃 Paper](https://www.arxiv.org/abs/2505.00752) -->
<!-- [📑 PDF](https://www.arxiv.org/pdf/2505.00752) -->
</div>
</div>


<!-- 合作论文按时间顺序排列 -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2025</div><img src='../../images/CSTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='CSTrack'></span>

**CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Li](https://github.com/Xuchen-Li), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
*[International Conference on Machine Learning](https://icml.cc/) (CCF-A Conference, Poster)*<br>
📌 Visual Object Tracking 📌 Multi-modal Learning <br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='../../images/CPDTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='CPDTrack'></span>

**Beyond Accuracy: Tracking more like Human via Visual Search**<br>
[D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [X. Li](https://github.com/Xuchen-Li), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
*[Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2024) (CCF-A Conference, Poster)*<br>
📌  Visual Object Tracking 📌 Visual Search Mechanism 📌 Visual Turing Test<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='../../images/MemVLT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MemVLT'></span>

**MemVLT: Vision-Language Tracking with Adaptive Memory-based Prompts**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [X. Li](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Hu</font>***, [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
*[Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2024) (CCF-A Conference, Poster)*<br>
📌 Visual Language Tracking 📌 Human-like Memory Modeling 📌 Adaptive Prompts<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRW 2024</div><img src='../../images/DTLLM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='DTLLM'></span>

**Diverse Text Generation for Visual Language Tracking Based on LLM**<br>
[X. Li](https://github.com/Xuchen-Li), [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), J. Zhang, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[the 3rd Workshop on Vision Datasets Understanding and DataCV Challenge in CVPR 2024](https://sites.google.com/view/vdu-cvpr24/) (Workshop in CCF-A Conference, Oral, Best Paper Honorable Mention)*<br>
📌 Visual Language Tracking 📌 Large Language Model 📌 Evaluation Technique<br>
[📃 Paper](https://arxiv.org/abs/2405.12139) 
[🗒 bibTex](https://huuuuusy.github.io/files/DTLLM-VLT.bib)
[📃 PDF](https://huuuuusy.github.io/files/DTLLM-VLT.pdf)
[🪧 Poster](https://github.com/Xuchen-Lifiles/DTLLM-poster.pdf)
[📹 Slides](https://github.com/Xuchen-Lifiles/DTLLM-Slides.pdf)
[🌐 Platform](http://videocube.aitestunion.com/)
[🔧 Toolkit](https://github.com/Xuchen-Li/DTLLM-VLT) 
[💾 Dataset](http://videocube.aitestunion.com/downloads)
[🏆 Award](https://huuuuusy.github.io/files/DTLLM-VLT-Award.pdf)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2025</div><img src='../../images/ICASSP25.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='ICASSP25'></span>

**Enhancing Vision-Language Tracking by Effectively Converting Textual Cues into Visual Cues**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Li](https://github.com/Xuchen-Li),  [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) <br>
*[IEEE International Conference on Acoustics, Speech, and Signal Processing](https://2025.ieeeicassp.org/) (CCF-B Conference, Poster)*<br>
📌 Visual Language Tracking 📌 Multi-modal Learning 📌 Grounding Model<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2024</div><img src='../../images/ICASSP24.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='ICASSP24'></span>

**Robust Single-particle Cryo-EM Image Denoising and Restoration**<br>
J. Zhang, T. Zhao, ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
*[IEEE International Conference on Acoustics, Speech, and Signal Processing](https://2024.ieeeicassp.org/) (CCF-B Conference, Poster)*<br>
📌 Medical Image Processing 📌 AI4Science 📌 Diffusion Model<br>
[📃 Paper](https://ieeexplore.ieee.org/abstract/document/10447135) 
[🗒 bibTex](https://huuuuusy.github.io/files/ICASSP24.bib)
[📑 PDF](https://huuuuusy.github.io/files/ICASSP24.pdf)

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TCSVT 2024</div><img src='../../images/AWCV.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='AWCV'></span>

**Finger in Camera Speaks Everything: Unconstrained Air-Writing for Real-World**<br>
[M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi), [Y. Cai](https://teacher.bupt.edu.cn/caiyuanqiang/zh_CN/index.htm), ***<font color=DarkRed>Shiyu Hu</font>***, [Y. Zhao](https://callsys.github.io/zhaoyuzhong.github.io-main/), [W. Wang](https://people.ucas.ac.cn/~wqwang?language=en) <br>
*[IEEE Transactions on Circuits and Systems for Video Technology](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76) (CCF-B Journal)*<br>
📌 Air-writing Technique 📌 Benchmark Construction 📌 Human-machine Interaction<br>
[📃 Paper](https://ieeexplore.ieee.org/document/10496279) 
[🗒 bibTex](https://huuuuusy.github.io/files/AWCV100k.bib)
[📃 PDF](https://huuuuusy.github.io/files/AWCV100k.pdf)
[🔧 Toolkit](https://github.com/wmeiqi/AWCV) 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PRCV 2024</div><img src='../../images/VSLLM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VSLLM'></span>

**VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing Projection Test**<br>
[M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), Y. Kang, [X. Li](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Hu</font>***, X. Chen, Y. kang, [W. Wang](https://people.ucas.ac.cn/~wqwang?language=en), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) <br>
*[Chinese Conference on Pattern Recognition and Computer Vision](https://www.prcv.cn) (CCF-C Conference)*<br>
📌 Psychological Assessment System 📌 Gamified Assessment 📌 AI4Science<br>
[📃 Paper](https://link.springer.com/chapter/10.1007/978-981-97-8692-3_17) 
[🗒 bibTex](https://huuuuusy.github.io/files/VSLLM.bib)
[📃 PDF](https://huuuuusy.github.io/files/VSLLM.pdf)

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PRCV 2023</div><img src='../../images/PRCV23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='PRCV23'></span>

**A Hierarchical Theme Recognition Model for Sandplay Therapy**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
*[Chinese Conference on Pattern Recognition and Computer Vision](https://www.prcv2023.cn/2023prcv) (CCF-C Conference, Poster)*<br>
📌 Psychological Assessment System 📌 Gamified Assessment 📌 AI4Science<br>
[📃 Paper](https://link.springer.com/chapter/10.1007/978-981-99-8462-6_20) 
[🗒 bibTex](https://huuuuusy.github.io/files/PRCV23.bib) 
[📑 PDF](https://huuuuusy.github.io/files/PRCV23.pdf)
[🔖 Supplementary](https://huuuuusy.github.io/files/PRCV23-Supp.pdf)
[🪧 Poster](https://huuuuusy.github.io/files/PRCV23-poster.pdf)

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Neurocomputing 2022</div><img src='../../images/Neu22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='Neu22'></span>

**Revisiting Instance Search: A New Benchmark Using Cycle Self-training**<br>
[Y. Zhang](https://wesleyzhang1991.github.io/), [C. Liu](https://scholar.google.com/citations?user=atOfOgMAAAAJ&hl=zh-CN&oi=sra), [W. Chen](https://scholar.google.com/citations?user=KWVlYaMAAAAJ&hl=zh-CN&oi=sra), [X. Xu](https://scholar.google.com/citations?user=nJc6BvgAAAAJ&hl=zh-CN&oi=sra), [F. Wang](https://scholar.google.com/citations?user=WCRGTHsAAAAJ), [H. Li](https://scholar.google.com/citations?user=pHN-QIwAAAAJ&hl=zh-CN&oi=sra), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
*[Neurocomputing](https://www.sciencedirect.com/journal/neurocomputing)  (CCF-C Journal)*<br>
📌 Video Instance Search 📌 Benchmark Construction  📌 Data Mining<br>
[📃 Paper](https://www.sciencedirect.com/science/article/abs/pii/S0925231222007445) 
[🗒 bibTex](https://huuuuusy.github.io/files/Neu22.bib) 
[📑 PDF](https://huuuuusy.github.io/files/Neu22.pdf) 
[🌐 Project](https://github.com/Instance-Search/) 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">图学学报 2021</div><img src='../../images/VTT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VTT'></span>

**Visual Turing: The Next Development of Computer Vision in The View of Human-computer Gaming (视觉图灵：从人机对抗看计算机视觉下一步发展)**<br>
[K. Huang](https://people.ucas.ac.cn/~huangkaiqi), [X. Zhao](https://www.xinzhaoai.com/), [Q. Li](https://scholar.google.com/citations?user=7xmxBagAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***<br>
*[Journal of Graphics](http://www.txxb.com.cn/CN/2095-302X/home.shtml) (《图学学报》, CCF-C Chinese Journal)*<br>
📌 Visual Object Tracking 📌 Intelligent Evaluation Technique  📌 AI4Science<br>
[📃 Paper](http://www.txxb.com.cn/CN/10.11996/JG.j.2095-302X.2021030339) 
[🗒 bibTex](https://huuuuusy.github.io/files/VTT.bib) 
[📑 PDF](https://huuuuusy.github.io/files/VTT.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">中国心理卫生杂志 2025</div><img src='../../images/IGBA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='IGBA'></span>

**A Review of Intelligent Psychological Assessment Based on Interactive Environment (基于交互环境的智能化心理测评)**<br>
[K. Huang](https://people.ucas.ac.cn/~huangkaiqi), Y. Kang, C. Yan, ***<font color=DarkRed>Shiyu Hu</font>***, [L. Wang](https://people.ucas.ac.cn/~wanglg), [T. Tao](https://people.ucas.ac.cn/~0072960), [W. Gao](https://people.ucas.ac.cn/~0000893) <br>
*[Chinese Mental Health Journal](http://xlwszz.tgcssci.com/) (《中国心理卫生杂志》, CSSCI Journal, Top Psychological Journal in China)*<br>
📌 Psychological Assessment System 📌 Gamified Assessment 📌 AI4Science<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CSAI 2023</div><img src='../../images/CSAI23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='CSAI23'></span>

**Rethinking Similar Object Interference in Single Object Tracking**<br>
[Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/)<br>
*[International Conference on Computer Science and Artificial Intelligence](http://www.csai.org/) (EI Conference, **Oral**)*<br>
📌 Visual Object Tracking 📌 Similar Object Interference 📌 Data Mining<br>
[📃 Paper](https://dl.acm.org/doi/abs/10.1145/3638584.3638644) 
[🗒 bibTex](https://huuuuusy.github.io/files/CSAI23.bib) 
[📑 PDF](https://huuuuusy.github.io/files/CSAI23.pdf)

</div>
</div>


## Preprint

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/FIOVA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='FIOVA'></span>

**FIOVA: A Multi-Annotator Benchmark for Human-Aligned Video Captioning**<br>
***<font color=DarkRed>Shiyu Hu</font>***\*, [X. Li\*](https://github.com/Xuchen-Li), [X. Li](https://github.com/XuzhaoLi), J. Zhang, [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), [X. Zhao](https://www.xinzhaoai.com/), [K. Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319) (*Equal Contributions)<br>
📌 Large Vision-Language Models 📌 Video Caption 📌 Video Understanding<br>
[📃 Paper](https://arxiv.org/abs/2410.15270) 
[📑 PDF](https://arxiv.org/pdf/2410.15270) 
[🌐 Project](https://huuuuusy.github.io/fiova/) 
<!-- [🗒 bibTex](https://huuuuusy.github.io/files/FIOVA.bib)  -->
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/SOE.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='SOE'></span>

**Students Rather Than Experts: A New AI for Education Pipeline to Model More Human-like and Personalised Early Adolescences**<br>
Y. Ma\*, ***<font color=DarkRed>Shiyu Hu</font>***\*, [X. Li](https://github.com/Xuchen-Li), [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), [S. Liu](https://faculty.ecnu.edu.cn/_s8/lsq/main.psp), [K. Cheong](https://dr.ntu.edu.sg/cris/rp/rp02319)  (*Equal Contributions) <br> 
📌 AI4Education 📌 LLMs 📌 LLM-based Agent<br>
[📃 Paper](https://arxiv.org/abs/2410.15701) 
[🗒 bibTex](https://huuuuusy.github.io/files/SOE.bib) 
[📑 PDF](https://arxiv.org/pdf/2410.15701) 
[🌐 Project](https://marsgemini.github.io/SOE-LVSA/) 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/DTVLT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='DTVLT'></span>

**DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM**<br>
[X. Li](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
📌 Visual Language Tracking 📌 Large Language Model 📌 Evaluation Technique<br>
[📃 Paper](https://arxiv.org/abs/2410.02492) 
[🗒 bibTex](https://huuuuusy.github.io/files/DTVLT.bib) 
[📑 PDF](https://arxiv.org/pdf/2410.02492) 
[🌐 Project](http://videocube.aitestunion.com/) 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/VLT-MI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VLT-MI'></span>

**Visual Language Tracking with Multi-modal Interaction: A Robust Benchmark**<br>
[X. Li](https://github.com/Xuchen-Li),  ***<font color=DarkRed>Shiyu Hu</font>***, [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) <br>
📌 Visual Language Tracking 📌 Multi-modal Interaction 📌 Evaluation Technology<br>
[📃 Paper](https://arxiv.org/abs/2409.08887) 
[🗒 bibTex](https://huuuuusy.github.io/files/VLT-MI.bib) 
[📑 PDF](https://arxiv.org/pdf/2409.08887) 
[🌐 Project](http://videocube.aitestunion.com/) 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-preprint">Preprint</div><img src='../../images/VTT-ICLR.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VTT-ICLR'></span>

**Nearing or Surpassing: Overall Evaluation of Human-Machine Dynamic Vision Ability**<br>
***<font color=DarkRed>Shiyu Hu</font>***, [X. Zhao](https://www.xinzhaoai.com/), [Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), [Y. Shan](https://scholar.google.com/citations?user=_nc83HsAAAAJ), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi) <br>
📌 Visual Object Tracking 📌 Intelligent Evaluation Technique 📌 AI4Science<br>
[📑 PDF](https://huuuuusy.github.io/files/VTT-ICLR.pdf)
[🗒 bibTex](https://huuuuusy.github.io/files/VTT-ICLR.bib) 
</div>
</div>

<!-- ## Under Review

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">TCSVT 2024</div><img src='../../images/SOI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='SOI'></span>

**Target or Distractor? Rethinking Similar Object Interference in Single Object Tracking**<br>
[Y. Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=nMe_kLAAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), [T. Yao](http://tingyao.deepfun.club/), [Y. Wang](https://scholar.google.com/citations?user=3nMDEBYAAAAJ), [L. Chen](https://sie.bit.edu.cn/szdw/jsml/ldjsyjsj/zgzcl/06c26b3ebaae4db981aaa388c660c8b5.htm), [X. Zhao](https://www.xinzhaoai.com/) <br>
*[IEEE Transactions on Circuits and Systems for Video Technology](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76) (CCF-B Journal, Under Review)*<br>
📌 Visual Object Tracking 📌 Similar Object Interference 📌 Data Mining<br>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">CCF-A 2024</div><img src='../../images/ATCTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='ATCTrack'></span>

**ATCTrack: Leveraging Aligned Target-Context Cues for Robust Vision-Language Tracking**<br>
[X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, [X. Li](https://github.com/Xuchen-Li), [D. Zhang](https://scholar.google.com.hk/citations?user=ApH4wOcAAAAJ), [M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), J. Zhang, X. Chen, [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)<br>
Submitted to a CCF-A conference, under review<br>
📌 Visual Language Tracking 📌 Multi-modal Alignment 📌 Feature Awareness<br>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">CCF-A 2024</div><img src='../../images/MMAW.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MMAW'></span>

**Unconstrained Multimodal Air-Writing Benchmark: Writing by Moving Your Fingers in 3D**<br>
[M. Wu](https://scholar.google.com.hk/citations?user=fGc7NVAAAAAJ), [X. Li](https://github.com/Xuchen-Li), ***<font color=DarkRed>Shiyu Hu</font>***, [Y. Cai](https://teacher.bupt.edu.cn/caiyuanqiang/zh_CN/index.htm), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi), [W. Wang](https://people.ucas.ac.cn/~wqwang?language=en) <br>
Submitted to a CCF-A conference, under review<br>
📌 Air-writing Technique 📌 Benchmark Construction 📌 Human-machine Interaction<br>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge-under-review">心理学报 2024</div><img src='../../images/Sandplay.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='Sandplay'></span>

**Intelligent Psychological Assessment with Sandplay based on Evidence-Centered Design Theory (基于证据中心设计理论的智能心理沙盘测评系统)**<br>
Y. Ren, [X. Feng](https://scholar.google.com.hk/citations?user=NqXtIPIAAAAJ), ***<font color=DarkRed>Shiyu Hu</font>***, Y. Kang, C. Yan, Y. Zeng, [L. Wang](https://people.ucas.ac.cn/~wanglg), [K. Huang](https://people.ucas.ac.cn/~huangkaiqi)  <br>
*[Acta Psychologica Sinica](https://journal.psych.ac.cn/xlxb/CN/0439-755X/home.shtml) (《心理学报》, CSSCI Journal, Top-1 Psychological Journal in China, Under Review)*<br>
📌 Psychological Assessment System 📌 Gamified Assessment 📌 AI4Science<br>

</div>
</div> -->